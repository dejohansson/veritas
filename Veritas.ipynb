{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm, markers\n",
    "import json\n",
    "from itertools import zip_longest, product\n",
    "import os\n",
    "from math import sqrt\n",
    "import holidays\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# instance_varibles\n",
    "# GLOBAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas display options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c:\\Users\\D\\veritas\n"
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"veritas_data/parser_menus/menu.json\", \"r\") as f:\n",
    "    MENU = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'veritas_data/stores/all.json'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b4427c9ce3df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mid_to_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mSTORE_ID_TO_NAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_store_id_to_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"veritas_data/stores/all.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-b4427c9ce3df>\u001b[0m in \u001b[0;36mget_store_id_to_name\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_store_id_to_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mraw_stores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mid_to_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Name\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_stores\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mid_to_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'veritas_data/stores/all.json'"
     ]
    }
   ],
   "source": [
    "def get_store_id_to_name(file_name):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_stores = json.load(f)\n",
    "        id_to_name = {store[\"Id\"] : store[\"Name\"] for store in raw_stores}\n",
    "    return id_to_name\n",
    "\n",
    "STORE_ID_TO_NAME = get_store_id_to_name(\"veritas_data/stores/all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'STORE_ID_TO_NAME' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6721fdd15dd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSTORE_ID_TO_NAME\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'STORE_ID_TO_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "STORE_ID_TO_NAME[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date, fiters out all dates before start_date\n",
    "# date_unit, determines the unit of the date column if date is a number\n",
    "def load_ts_data(file_name, start_date=None, date_unit=None):\n",
    "    print(\"Loading main columns...\")\n",
    "    df = pd.read_csv(file_name, usecols=[\"device_id\", \"date\", \"store_id\", \"class\", \"total_price\"], header=0)\n",
    "\n",
    "    #print(\"Formatting main columns...\")\n",
    "    #df[\"device_id\"], device_map = df[\"device_id\"].factorize()\n",
    "    #df[\"store_id\"], store_map = df[\"store_id\"].factorize()\n",
    "    #df[\"class\"], class_map = df[\"class\"].factorize()\n",
    "    #df = df.astype(\"uint32\")\n",
    "\n",
    "    print(\"Loading products...\")\n",
    "    with open(file_name, \"r\") as f:\n",
    "        num_columns = len(f.readline().split(\",\"))\n",
    "    for cols in zip_longest(*(iter(range(5, num_columns)),) * 50):\n",
    "        cols = [c for c in cols if c is not None]\n",
    "        temp_df = pd.read_csv(file_name, usecols=list(cols), header=0, dtype=\"uint8\")\n",
    "        temp_df = temp_df.loc[:, (temp_df != 0).any(axis=0)]   # Remove \"zero\" columns\n",
    "        df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "    wl = [col for col in df.columns if col not in ['device_id', 'store_id', 'class', \"date\"]]\n",
    "    df = df.drop(df[df[wl].eq(0).all(axis=1)].index)\n",
    "    if date_unit is None:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], unit=date_unit)\n",
    "    if start_date is not None:\n",
    "            df.drop(df[df[\"date\"] < start_date].index, inplace=True)\n",
    "    df.set_index([\"store_id\", \"device_id\", \"date\"], inplace=True)\n",
    "    return df#, device_map, store_map, class_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading main columns...\nLoading products...\n"
    }
   ],
   "source": [
    "TS_FILE = \"veritas_data/post_parser_orders/device_time_series_2020_01-01_to_02-19.csv\"\n",
    "TSF = load_ts_data(TS_FILE, start_date=\"2020-01-01\", date_unit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSF.drop(\"class\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['total_price', 'product_4', 'product_5', 'product_6', 'product_8', 'product_9', 'product_10', 'product_11', 'product_12', 'product_14',\n       ...\n       'product_392', 'product_393', 'product_394', 'product_395', 'product_396', 'product_397', 'product_398', 'product_400', 'product_401', 'product_402'], dtype='object', length=196)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "TSF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COL_WL = [col for col in TSF.columns if col not in [\"device_id\", \"store_id\", \"class\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, path):\n",
    "    if not os.path.isfile(path):\n",
    "        df.to_csv(path)\n",
    "    else:\n",
    "        print(path, \"already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns the device_id, number of orders and the data frame of the device with the most orders\n",
    "def get_top_devices(device_id_list, ts_frame, n):\n",
    "    top_devices = []\n",
    "    for device_id in device_id_list:\n",
    "        df = ts_frame.loc[ts_frame[\"device_id\"] == device_id, [\"store_id\"]]\n",
    "        try:\n",
    "            store_id = df.values[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        num_orders = len(df.index)\n",
    "        top_devices.append((device_id, num_orders, store_id))\n",
    "        top_devices = sorted(top_devices, key=lambda d: d[1], reverse=True)[:n]\n",
    "    return top_devices\n",
    "\n",
    "# Returns df without rows where col_name equals a value occuring less than threshold times\n",
    "def trim_low_occurance_values(df, col_name, threshold):\n",
    "    s = df[col_name].value_counts().ge(threshold)\n",
    "    return df[df[col_name].isin(s[s].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "def get_intervals(df, t_delta):\n",
    "    first_date = df.index.min()[2]\n",
    "    last_date = df.index.max()[2]\n",
    "    time_step = last_date-t_delta\n",
    "    intervals = []\n",
    "    while time_step > first_date:\n",
    "        start = time_step\n",
    "        end = time_step+t_delta\n",
    "        intervals.append((start, end))\n",
    "        time_step -= (t_delta*max(min(np.random.normal(0.5, 0.05), 1), 0.4))\n",
    "    intervals.append((first_date, first_date+t_delta))\n",
    "    return intervals\n",
    "\n",
    "def sparsity_coefficient(df):\n",
    "    return 1.0 - (np.count_nonzero(df) / np.product(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "def reindex_by_date(df, start, end, freq, fill_value=0):\n",
    "    dates = pd.date_range(start=start, end=end, freq=freq)\n",
    "    index = list(set(df.index.droplevel(\"date\").tolist()))\n",
    "    index.sort()\n",
    "    dates = [(i, j, d) for i, j in index for d in dates]\n",
    "    df = df.reindex(dates).fillna(0)\n",
    "    return df\n",
    "\n",
    "def flatten_ts(df):\n",
    "    return df.values.flatten()\n",
    "\n",
    "# \n",
    "def format_ts(df, start, end, freq, whitelist=[], flatten=False, normalize=False, sales_percentage=False):\n",
    "    product_cols = [col for col in df.columns if \"product\" in col]\n",
    "    df.rename(columns={\"total_price\": \"average_price\"}, inplace=True)\n",
    "    df[\"num_orders\"] = 1\n",
    "    agg_dict = {col_name: np.sum for col_name in df.columns}\n",
    "    agg_dict.update({\"average_price\": np.mean})\n",
    "    df = df.groupby([df.index.get_level_values(i) for i in [0,1]]+[pd.Grouper(freq=freq, level=-1)]).agg(agg_dict)\n",
    "    df = reindex_by_date(df, df.index.get_level_values(\"date\")[0], df.index.get_level_values(\"date\")[-1], freq)\n",
    "    if sales_percentage:\n",
    "        df[product_cols] = df[product_cols].div(df[product_cols].sum(axis=1), axis=0)\n",
    "    if normalize:\n",
    "        df -= df.min()\n",
    "        df /= df.max()\n",
    "    df = df.fillna(0)\n",
    "    if flatten:\n",
    "        df.index.droplevel(\"store_id\")\n",
    "        device_index = df.index.droplevel(\"date\").drop_duplicates()\n",
    "        df = df.groupby(level=\"device_id\").apply(flatten_ts)\n",
    "        df = pd.DataFrame(np.stack(df), index=device_index)\n",
    "    return df\n",
    "\n",
    "def correlation_matrix(df, start, end, freq, whitelist, dist_func, normalize=False, sales_percentage=False):\n",
    "    start_time = time.time()\n",
    "    ids = df.index.get_level_values(\"device_id\").unique()\n",
    "    err_df = pd.DataFrame(index=ids, columns=ids)\n",
    "\n",
    "    print(\"Formatting devices...\")\n",
    "    df = format_ts(df, start, end, freq, whitelist, normalize=normalize, sales_percentage=sales_percentage, flatten=True)\n",
    "\n",
    "    print(\"Generating correlation matrix...\")\n",
    "    err_df = pd.DataFrame(squareform(pdist(df, metric=\"euclidean\")), columns=df.index, index=df.index)\n",
    "    print(\"Done!\", \"\\tElapsed time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Score Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cophenetic Testing\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, cophenet\n",
    "\n",
    "def cophenet_test(df, intervals, freqs, metrics, methods, pcas, whitelist=[], verbose=False):\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    if verbose:\n",
    "        print(\"Running tests...\")\n",
    "    windows = [(l, w) for l in intervals for w in get_intervals(df, l)]\n",
    "    n_tests = len(windows)*len(freqs)*len(metrics)*len(methods)*len(pcas)\n",
    "    if n_tests == 0:\n",
    "        print(\"Parameters generated zero tests!!!\")\n",
    "        return\n",
    "    tests_completed = 0\n",
    "\n",
    "    for interval, (start, end) in windows:\n",
    "        window = df[(df.index.get_level_values(\"date\") > start) & (df.index.get_level_values(\"date\") <= end)]\n",
    "        n_devices = window.index.get_level_values(\"device_id\").nunique()\n",
    "        if n_devices > 1:\n",
    "            for freq in freqs:\n",
    "                formatted_window = format_ts(window, start, end, freq, whitelist, flatten=True, normalize=True)\n",
    "                for n_pca in pcas:\n",
    "                    try:\n",
    "                        window_components = TruncatedSVD(n_components=n_pca).fit_transform(formatted_window)\n",
    "                        for metric in metrics:\n",
    "                            dist = pdist(window_components, metric=metric)\n",
    "                            for method in methods:\n",
    "                                if method != \"ward\" or metric == \"euclidean\":\n",
    "                                    link = linkage(dist, method=method)\n",
    "                                    score = cophenet(link, dist)[0]\n",
    "                                    scores.append((interval, freq, metric, method, n_pca, score))\n",
    "                                tests_completed += 1\n",
    "                                if verbose:\n",
    "                                    time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                                    print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "                    except ValueError:\n",
    "                        break\n",
    "        else:\n",
    "            tests_completed += n_tests//len(windows)\n",
    "            if verbose:\n",
    "                time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "                            \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\nDone! Total time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store based clustering\n",
    "# 100 PCA!\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def store_clust_test(df, intervals, freqs, verbose=False):\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    if verbose:\n",
    "        print(\"Running tests...\")\n",
    "    windows = [(l, w) for l in intervals for w in get_intervals(df, l)]\n",
    "    n_tests = len(windows)*len(freqs)\n",
    "    if n_tests == 0:\n",
    "        print(\"Parameters generated zero tests!!!\")\n",
    "        return\n",
    "    tests_completed = 0\n",
    "\n",
    "    for interval, (start, end) in windows:\n",
    "        window = df[(df.index.get_level_values(\"date\") > start) & (df.index.get_level_values(\"date\") <= end)]\n",
    "        n_devices = window.index.get_level_values(\"device_id\").nunique()\n",
    "        if n_devices > 1:\n",
    "            for freq in freqs:\n",
    "                formatted_window = format_ts(window, start, end, freq, [], flatten=True, normalize=True)\n",
    "                store_clust = formatted_window.index.get_level_values(\"store_id\")        \n",
    "                window_components = TruncatedSVD(n_components=100).fit_transform(formatted_window)\n",
    "                dist = pdist(window_components, metric=\"euclidean\")\n",
    "                si_score = ch_score = db_score = np.NINFnp.NINF\n",
    "                for n_clust in range(2, min(100, n_devices//2)):\n",
    "                    si = silhouette_score(squareform(dist), store_clust)\n",
    "                    if si > si_score:\n",
    "                        si_score = si\n",
    "                    ch = calinski_harabasz_score(squareform(dist), store_clust)\n",
    "                    if ch > ch_score:\n",
    "                        ch_score = ch\n",
    "                    db = davies_bouldin_score(squareform(dist), store_clust)\n",
    "                    if db > db_score:\n",
    "                        db_score = db\n",
    "                scores.append((interval, freq, si_score, ch_score, db_score))\n",
    "                tests_completed += 1\n",
    "                if verbose:\n",
    "                    time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                    print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "        else:\n",
    "            tests_completed += n_tests//len(windows)\n",
    "            if verbose:\n",
    "                time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "                            \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\nDone! Total time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "def k_means(x, n_clust):\n",
    "    return KMeans(n_clusters=n_clust, random_state=0, n_jobs=-1, verbose=2).fit(x).labels_\n",
    "\n",
    "def optics(x, min_samples):\n",
    "    return OPTICS(min_samples=min_samples, metric=\"euclidean\").fit(x).labels_\n",
    "\n",
    "def agglomerative_complete(x, n_clust):\n",
    "    dist = pdist(x, metric=\"euclidean\")\n",
    "    link = linkage(dist, method=\"complete\")\n",
    "    return fcluster(link, t=n_clust, criterion=\"maxclust\").astype(\"float64\")\n",
    "\n",
    "def agglomerative_single(x, n_clust):\n",
    "    dist = pdist(x, metric=\"euclidean\")\n",
    "    link = linkage(dist, method=\"single\")\n",
    "    return fcluster(link, t=n_clust, criterion=\"maxclust\").astype(\"float64\")\n",
    "\n",
    "def agglomerative_ward(x, n_clust):\n",
    "    dist = pdist(x, metric=\"euclidean\")\n",
    "    link = linkage(dist, method=\"ward\")\n",
    "    return fcluster(link, t=n_clust, criterion=\"maxclust\").astype(\"float64\")\n",
    "    \n",
    "def store_clust(x):\n",
    "    return x.index.get_level_values(\"store_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store based clustering\n",
    "# 100 PCA!\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def clust_test(df, intervals, freqs, clust_funcs, verbose=False):\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    if verbose:\n",
    "        print(\"Running tests...\")\n",
    "    windows = [(l, w) for l in intervals for w in get_intervals(df, l)]\n",
    "    n_tests = len(windows)*len(freqs)*len(clust_funcs)\n",
    "    if n_tests == 0:\n",
    "        print(\"Parameters generated zero tests!!!\")\n",
    "        return\n",
    "    tests_completed = 0\n",
    "\n",
    "    for interval, (start, end) in windows:\n",
    "        window = df[(df.index.get_level_values(\"date\") > start) & (df.index.get_level_values(\"date\") <= end)]\n",
    "        n_devices = window.index.get_level_values(\"device_id\").nunique()\n",
    "        if n_devices > 1:\n",
    "            for freq in freqs:\n",
    "                formatted_window = format_ts(window, start, end, freq, [], flatten=True, normalize=True)    \n",
    "                svd = TruncatedSVD(n_components=min(1000, formatted_window.shape[1]-1), random_state=0)\n",
    "                svd.fit(formatted_window)\n",
    "                variance = svd.explained_variance_ratio_.cumsum()\n",
    "                window_components = svd.transform(formatted_window)[:,variance < 0.96]\n",
    "                n_components = window_components.shape[1]\n",
    "                dist = pdist(window_components, metric=\"euclidean\")\n",
    "                for clust_func in clust_funcs:\n",
    "                    si_score = ch_score = np.NINF \n",
    "                    db_score = np.inf\n",
    "                    si_n_clust = ch_n_clust = db_n_clust = None\n",
    "                    si_n_outliers = ch_n_outliers = db_n_outliers = None\n",
    "                    if clust_func == optics:\n",
    "                        for min_samples in range(2, min(100, n_devices//20), 2):\n",
    "                            clust_labels = clust_func(window_components, min_samples)\n",
    "                            mask = clust_labels != -1\n",
    "                            n_outliers = np.count_nonzero(clust_labels == -1)\n",
    "                            clust_labels = clust_labels[mask]\n",
    "                            n_clust = len(np.unique(clust_labels))\n",
    "                            if n_clust < 2 : continue\n",
    "                            si = silhouette_score(squareform(dist)[mask][:,mask], clust_labels)\n",
    "                            if si > si_score:\n",
    "                                si_score = si\n",
    "                                si_n_clust = n_clust\n",
    "                                si_n_outliers = n_outliers\n",
    "                            ch = calinski_harabasz_score(squareform(dist)[mask][:,mask], clust_labels)\n",
    "                            if ch > ch_score:\n",
    "                                ch_score = ch\n",
    "                                ch_n_clust = n_clust\n",
    "                                ch_n_outliers = n_outliers\n",
    "                            db = davies_bouldin_score(squareform(dist)[mask][:,mask], clust_labels)\n",
    "                            if db < db_score:\n",
    "                                db_score = db\n",
    "                                db_n_clust = n_clust\n",
    "                                db_n_outliers = n_outliers\n",
    "                    elif clust_func == store_clust:\n",
    "                        clust_labels = clust_func(formatted_window)\n",
    "                        si_score = silhouette_score(squareform(dist), clust_labels)\n",
    "                        ch_score = calinski_harabasz_score(squareform(dist), clust_labels)\n",
    "                        db_score = davies_bouldin_score(squareform(dist), clust_labels)\n",
    "                        si_n_clust = ch_n_clust = db_n_clust = len(np.unique(clust_labels))\n",
    "                    else:\n",
    "                        for n_clust in range(2, min(100, n_devices//2)):\n",
    "                            clust_labels = clust_func(window_components, n_clust)\n",
    "                            si = silhouette_score(squareform(dist), clust_labels)\n",
    "                            if si > si_score:\n",
    "                                si_score = si\n",
    "                                si_n_clust = n_clust\n",
    "                            ch = calinski_harabasz_score(squareform(dist), clust_labels)\n",
    "                            if ch > ch_score:\n",
    "                                ch_score = ch\n",
    "                                ch_n_clust = n_clust\n",
    "                            db = davies_bouldin_score(squareform(dist), clust_labels)\n",
    "                            if db < db_score:\n",
    "                                db_score = db\n",
    "                                db_n_clust = n_clust\n",
    "                    scores.append((interval, freq, clust_func.__name__, si_score, ch_score, db_score, si_n_clust, ch_n_clust, db_n_clust, n_components, si_n_outliers, ch_n_outliers, db_n_outliers))\n",
    "                    tests_completed += 1\n",
    "                    if verbose:\n",
    "                        time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                        print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "        else:\n",
    "            tests_completed += n_tests//len(windows)\n",
    "            if verbose:\n",
    "                time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")        \n",
    "    if verbose:\n",
    "        print(\"\\nDone! Total time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "844 844\n844 601\n"
    }
   ],
   "source": [
    "start, end = get_intervals(TSF, pd.Timedelta(1, unit=\"D\"))[0]\n",
    "\n",
    "window = TSF[(TSF.index.get_level_values(\"date\") > start) & (TSF.index.get_level_values(\"date\") <= end)]\n",
    "formatted_window = format_ts(window, start, end, \"1H\", [], flatten=True, normalize=True)\n",
    "formatted_window.shape    \n",
    "svd = TruncatedSVD(n_components=min(1000, formatted_window.shape[1]-1), random_state=0)\n",
    "svd.fit(formatted_window)\n",
    "variance = svd.explained_variance_ratio_.cumsum()\n",
    "print(len(variance), len(svd.transform(formatted_window)))\n",
    "window_components = svd.transform(formatted_window)[:,(variance < 0.96)]\n",
    "n_components = len(window_components)\n",
    "print(svd.transform(formatted_window).shape[1], window_components.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 3, 7, 14, 48 Days\n",
    "# 1, 12, 24, Hours\n",
    "\n",
    "TEST_NAME = \"6-methods_interval-1D-28D_freq-1H-24H_SVD-95-percent\"\n",
    "TEST_RES_PATH = \"./test_results/\" + TEST_NAME + \".csv\"\n",
    "#TEST_INDEX = [\"interval\", \"frequency\", \"metric\", \"method\", \"n_pca\"]\n",
    "TEST_INDEX = [\"interval\", \"frequency\", \"clust_func\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Running tests...\nTest: 2790/2790 \tTime remaining (est): 0:00:00\nDone! Total time: 1 day, 7:40:29.904147\n"
    }
   ],
   "source": [
    "FREQ_UNIT = \"H\"\n",
    "INTERVALS = [pd.Timedelta(n, unit=\"D\") for n in [1, 3, 7, 14, 28]]\n",
    "FREQS = [str(n)+FREQ_UNIT for n in [1, 12, 24]]\n",
    "DIST_METRICS = [\"euclidean\"]\n",
    "LINK_METHODS = [\"average\"]\n",
    "PCAS = [20, 60, 100, 200, 300]\n",
    "METHODS = [k_means, optics, agglomerative_complete, agglomerative_ward, agglomerative_single, store_clust]\n",
    "\n",
    "TEST_RES = clust_test(TSF, INTERVALS, FREQS, METHODS, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF = pd.DataFrame(TEST_RES, columns=TEST_INDEX+[\"si_score\", \"ch_score\", \"db_score\", \"si_n_clust\", \"ch_n_clust\", \"db_n_clust\", \"n_components\", \"si_n_outliers\", \"ch_n_outliers\", \"db_n_outliers\"])\n",
    "TEST_DF.set_index(TEST_INDEX, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           si_score    ch_score   db_score  si_n_clust  ch_n_clust  db_n_clust  n_components  si_n_outliers  ch_n_outliers  db_n_outliers\ninterval frequency clust_func                                                                                                                                            \n1 days   1H        k_means                 0.437678  937.381589   0.793580         2.0         2.0         2.0           601            NaN            NaN            NaN\n                   optics                      -inf        -inf        inf         NaN         NaN         NaN           601            NaN            NaN            NaN\n                   agglomerative_complete  0.619589   35.106322   0.223223         3.0         5.0         3.0           601            NaN            NaN            NaN\n                   agglomerative_ward      0.479535  937.354184   0.713367         2.0         3.0         2.0           601            NaN            NaN            NaN\n                   agglomerative_single    0.686919   17.907900   0.198211         2.0         2.0         2.0           601            NaN            NaN            NaN\n...                                             ...         ...        ...         ...         ...         ...           ...            ...            ...            ...\n28 days  24H       optics                  0.363113    5.123374   0.862588         2.0         2.0         2.0           683          851.0          851.0          851.0\n                   agglomerative_complete  0.608758  286.764474   0.602891         2.0         4.0         2.0           683            NaN            NaN            NaN\n                   agglomerative_ward      0.425747  895.274004   0.792692         2.0         2.0         2.0           683            NaN            NaN            NaN\n                   agglomerative_single    0.777901   32.800680   0.145113         2.0         2.0         2.0           683            NaN            NaN            NaN\n                   store_clust            -0.473466    1.108571  13.107213       125.0       125.0       125.0           683            NaN            NaN            NaN\n\n[2790 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>si_score</th>\n      <th>ch_score</th>\n      <th>db_score</th>\n      <th>si_n_clust</th>\n      <th>ch_n_clust</th>\n      <th>db_n_clust</th>\n      <th>n_components</th>\n      <th>si_n_outliers</th>\n      <th>ch_n_outliers</th>\n      <th>db_n_outliers</th>\n    </tr>\n    <tr>\n      <th>interval</th>\n      <th>frequency</th>\n      <th>clust_func</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1 days</th>\n      <th rowspan=\"5\" valign=\"top\">1H</th>\n      <th>k_means</th>\n      <td>0.437678</td>\n      <td>937.381589</td>\n      <td>0.793580</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>601</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>601</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_complete</th>\n      <td>0.619589</td>\n      <td>35.106322</td>\n      <td>0.223223</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>601</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.479535</td>\n      <td>937.354184</td>\n      <td>0.713367</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>601</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.686919</td>\n      <td>17.907900</td>\n      <td>0.198211</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>601</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">28 days</th>\n      <th rowspan=\"5\" valign=\"top\">24H</th>\n      <th>optics</th>\n      <td>0.363113</td>\n      <td>5.123374</td>\n      <td>0.862588</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>683</td>\n      <td>851.0</td>\n      <td>851.0</td>\n      <td>851.0</td>\n    </tr>\n    <tr>\n      <th>agglomerative_complete</th>\n      <td>0.608758</td>\n      <td>286.764474</td>\n      <td>0.602891</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.425747</td>\n      <td>895.274004</td>\n      <td>0.792692</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.777901</td>\n      <td>32.800680</td>\n      <td>0.145113</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.473466</td>\n      <td>1.108571</td>\n      <td>13.107213</td>\n      <td>125.0</td>\n      <td>125.0</td>\n      <td>125.0</td>\n      <td>683</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>2790 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "TEST_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(TEST_DF, TEST_RES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF = pd.read_csv(TEST_RES_PATH)\n",
    "#TEST_DF = pd.read_csv(\"./test_results/\" + \"frequency_test_2W_30min_to_10080min\" + \".csv\")\n",
    "\n",
    "#TEST_DF = pd.concat([pd.read_csv(\"./test_results/\" + \"frequency_test_1W_30min_to_4320min\" + \".csv\"), pd.read_csv(\"./test_results/\" + \"frequency_test_1W_4350min_to_10080min\" + \".csv\")])\n",
    "TEST_DF.set_index(TEST_INDEX, inplace=True)\n",
    "TEST_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                           si_score     ch_score   db_score  si_n_clust  ch_n_clust  db_n_clust  n_components  si_n_outliers  ch_n_outliers  db_n_outliers\ninterval frequency clust_func                                                                                                                                             \n1 days   12H       agglomerative_complete  0.623797   146.236750   0.420281    2.141414    4.191919    2.444444    257.797980            NaN            NaN            NaN\n                   agglomerative_single    0.704312    22.654634   0.185554    2.373737    2.595960    2.454545    257.797980            NaN            NaN            NaN\n                   agglomerative_ward      0.396977   684.754070   0.877321    2.020202    2.282828    2.121212    257.797980            NaN            NaN            NaN\n                   k_means                 0.445740   896.806330   0.829006    2.000000    2.040404    2.040404    257.797980            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.852459    2.852459    2.852459    257.797980     826.426230     826.344262     826.426230\n                   store_clust            -0.446528     0.987967  11.574773  124.101010  124.101010  124.101010    257.797980            NaN            NaN            NaN\n         1H        agglomerative_complete  0.585759    83.599234   0.370169    2.222222    5.222222    4.000000    596.808081            NaN            NaN            NaN\n                   agglomerative_single    0.661750    17.004771   0.212829    2.080808    2.171717    2.090909    596.808081            NaN            NaN            NaN\n                   agglomerative_ward      0.413867   820.678791   0.827009    2.090909    2.606061    6.383838    596.808081            NaN            NaN            NaN\n                   k_means                 0.425849   831.248277   0.741595    2.090909    2.272727    9.666667    596.808081            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.600000    2.600000    2.600000    596.808081     828.000000     828.000000     828.000000\n                   store_clust            -0.475728     0.971255  13.100335  124.101010  124.101010  124.101010    596.808081            NaN            NaN            NaN\n         24H       agglomerative_complete  0.633894   175.790316   0.427488    2.090909    3.737374    2.181818    177.787879            NaN            NaN            NaN\n                   agglomerative_single    0.710104    23.594297   0.181980    2.454545    2.646465    2.494949    177.787879            NaN            NaN            NaN\n                   agglomerative_ward      0.398989   683.741942   0.887107    2.010101    2.252525    2.161616    177.787879            NaN            NaN            NaN\n                   k_means                 0.447155   893.275408   0.836087    2.000000    2.090909    2.030303    177.787879            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    3.615385    3.615385    3.615385    177.787879     824.961538     824.961538     824.961538\n                   store_clust            -0.437231     0.990746  11.196497  124.101010  124.101010  124.101010    177.787879            NaN            NaN            NaN\n3 days   12H       agglomerative_complete  0.635210   187.607048   0.429112    2.031250    3.031250    2.250000    462.968750            NaN            NaN            NaN\n                   agglomerative_single    0.724681    25.663587   0.174352    2.312500    2.375000    2.343750    462.968750            NaN            NaN            NaN\n                   agglomerative_ward      0.417461   842.404190   0.815893    2.031250    2.281250    2.125000    462.968750            NaN            NaN            NaN\n                   k_means                 0.474212  1042.178723   0.774211    2.000000    2.000000    2.062500    462.968750            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.000000    2.000000    2.000000    462.968750     842.200000     842.200000     842.200000\n                   store_clust            -0.458444     0.977608  12.076956  124.375000  124.375000  124.375000    462.968750            NaN            NaN            NaN\n         1H        agglomerative_complete  0.556285    86.095992   0.399960    3.906250    5.125000    5.687500    696.562500            NaN            NaN            NaN\n                   agglomerative_single    0.667593    17.796566   0.210109    2.031250    2.125000    2.031250    696.562500            NaN            NaN            NaN\n                   agglomerative_ward      0.393620   843.800244   0.840791    2.156250    2.750000    5.343750    696.562500            NaN            NaN            NaN\n                   k_means                 0.441683   895.855237   0.703650    2.125000    2.250000   12.250000    696.562500            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.000000    2.000000    2.000000    696.562500     839.250000     839.250000     839.250000\n                   store_clust            -0.474911     0.949261  13.336408  124.375000  124.375000  124.375000    696.562500            NaN            NaN            NaN\n         24H       agglomerative_complete  0.645298   251.796994   0.409973    2.187500    3.625000    2.187500    338.312500            NaN            NaN            NaN\n                   agglomerative_single    0.739415    28.415144   0.164843    2.281250    2.250000    2.250000    338.312500            NaN            NaN            NaN\n                   agglomerative_ward      0.444756   858.666361   0.799871    2.031250    2.343750    2.125000    338.312500            NaN            NaN            NaN\n                   k_means                 0.480562  1033.821318   0.777096    2.000000    2.187500    2.031250    338.312500            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.000000    2.000000    2.000000    338.312500     838.125000     838.125000     838.125000\n                   store_clust            -0.452745     0.982078  11.836771  124.375000  124.375000  124.375000    338.312500            NaN            NaN            NaN\n7 days   12H       agglomerative_complete  0.621630   181.953754   0.420023    2.071429    4.357143    2.714286    592.928571            NaN            NaN            NaN\n                   agglomerative_single    0.730381    26.584817   0.171315    2.142857    2.142857    2.142857    592.928571            NaN            NaN            NaN\n                   agglomerative_ward      0.422760   862.347214   0.817751    2.000000    2.071429    2.071429    592.928571            NaN            NaN            NaN\n                   k_means                 0.482133  1114.683170   0.750728    2.000000    2.071429    2.000000    592.928571            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.333333    2.333333    2.333333    592.928571     844.000000     844.000000     844.000000\n                   store_clust            -0.467556     0.963544  12.394957  124.714286  124.714286  124.714286    592.928571            NaN            NaN            NaN\n         1H        agglomerative_complete  0.629730    67.303724   0.294955    2.000000    4.357143    2.357143    725.428571            NaN            NaN            NaN\n                   agglomerative_single    0.661559    17.220069   0.214293    2.000000    2.000000    2.000000    725.428571            NaN            NaN            NaN\n                   agglomerative_ward      0.404066   872.840890   0.819001    2.071429    2.142857    2.000000    725.428571            NaN            NaN            NaN\n                   k_means                 0.457032   900.184541   0.616099    2.000000    2.142857    2.642857    725.428571            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.111111    2.111111    2.111111    725.428571     841.222222     841.222222     841.222222\n                   store_clust            -0.474369     0.937278  13.449668  124.714286  124.714286  124.714286    725.428571            NaN            NaN            NaN\n         24H       agglomerative_complete  0.598556   332.836766   0.512786    2.214286    3.071429    9.285714    499.857143            NaN            NaN            NaN\n                   agglomerative_single    0.748071    29.153764   0.162045    2.071429    2.357143    2.071429    499.857143            NaN            NaN            NaN\n                   agglomerative_ward      0.449453   896.549292   0.780988    2.071429    2.357143    2.142857    499.857143            NaN            NaN            NaN\n                   k_means                 0.488117  1097.164652   0.754173    2.000000    2.142857    2.000000    499.857143            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    3.000000    3.000000    3.000000    499.857143     843.666667     843.666667     843.666667\n                   store_clust            -0.465721     0.967419  12.176733  124.714286  124.714286  124.714286    499.857143            NaN            NaN            NaN\n14 days  12H       agglomerative_complete  0.609651   106.208618   0.466463    2.000000    3.285714    2.714286    678.571429            NaN            NaN            NaN\n                   agglomerative_single    0.733344    26.562779   0.169923    2.142857    2.142857    2.142857    678.571429            NaN            NaN            NaN\n                   agglomerative_ward      0.442391   941.583276   0.777610    2.000000    2.142857    2.000000    678.571429            NaN            NaN            NaN\n                   k_means                 0.488102  1166.282396   0.732030    2.000000    2.000000    2.000000    678.571429            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.000000    2.000000    2.000000    678.571429     848.666667     848.666667     848.666667\n                   store_clust            -0.479499     1.049868  12.803594  125.000000  125.000000  125.000000    678.571429            NaN            NaN            NaN\n         1H        agglomerative_complete  0.569308    57.542639   0.428236    2.000000    4.714286    2.571429    744.285714            NaN            NaN            NaN\n                   agglomerative_single    0.663442    16.481117   0.213742    2.000000    2.000000    2.000000    744.285714            NaN            NaN            NaN\n                   agglomerative_ward      0.395519   893.285108   0.886096    2.142857    2.428571    2.000000    744.285714            NaN            NaN            NaN\n                   k_means                 0.444943   845.995686   0.684873    2.000000    2.285714    8.428571    744.285714            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.166667    2.166667    2.166667    744.285714     846.833333     846.833333     846.833333\n                   store_clust            -0.472167     1.011976  13.690378  125.000000  125.000000  125.000000    744.285714            NaN            NaN            NaN\n         24H       agglomerative_complete  0.551674   342.566766   0.595995    2.000000    2.714286    3.714286    624.714286            NaN            NaN            NaN\n                   agglomerative_single    0.732241    28.545943   0.166812    2.285714    2.285714    2.285714    624.714286            NaN            NaN            NaN\n                   agglomerative_ward      0.443305   968.117513   0.770155    2.000000    2.142857    2.000000    624.714286            NaN            NaN            NaN\n                   k_means                 0.498118  1154.582303   0.729925    2.000000    2.142857    2.000000    624.714286            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.333333    2.333333    2.333333    624.714286     846.333333     846.333333     846.333333\n                   store_clust            -0.473607     1.048759  12.697083  125.000000  125.000000  125.000000    624.714286            NaN            NaN            NaN\n28 days  12H       agglomerative_complete  0.642518   115.507702   0.431183    2.000000    2.666667    2.000000    719.666667            NaN            NaN            NaN\n                   agglomerative_single    0.744751    25.801052   0.164648    2.000000    2.000000    2.000000    719.666667            NaN            NaN            NaN\n                   agglomerative_ward      0.433331   973.089162   0.790731    2.000000    2.000000    2.000000    719.666667            NaN            NaN            NaN\n                   k_means                 0.485615  1177.161530   0.729206    2.000000    2.000000    2.000000    719.666667            NaN            NaN            NaN\n                   optics                      -inf         -inf        inf    2.000000    2.000000    2.000000    719.666667     854.000000     854.000000     854.000000\n                   store_clust            -0.460550     1.012619  13.222149  125.000000  125.000000  125.000000    719.666667            NaN            NaN            NaN\n         1H        agglomerative_complete  0.561293    51.244228   0.459199    2.000000    3.000000    2.000000    756.333333            NaN            NaN            NaN\n                   agglomerative_single    0.671023    15.820627   0.209331    2.000000    2.000000    2.000000    756.333333            NaN            NaN            NaN\n                   agglomerative_ward      0.391923   858.314201   0.830534    2.000000    2.000000    2.000000    756.333333            NaN            NaN            NaN\n                   k_means                 0.502379   718.782795   0.574381    2.000000    2.666667    3.666667    756.333333            NaN            NaN            NaN\n                   optics                  0.411184    10.519791   0.690513    3.666667    3.666667    3.666667    756.333333     848.666667     848.666667     848.666667\n                   store_clust            -0.454928     0.978977  13.803514  125.000000  125.000000  125.000000    756.333333            NaN            NaN            NaN\n         24H       agglomerative_complete  0.517136   358.034690   0.696998    2.000000    2.666667    2.333333    692.666667            NaN            NaN            NaN\n                   agglomerative_single    0.738454    27.435288   0.163044    2.666667    2.333333    2.666667    692.666667            NaN            NaN            NaN\n                   agglomerative_ward      0.413781   862.135546   0.788118    2.000000    2.000000    2.000000    692.666667            NaN            NaN            NaN\n                   k_means                 0.502071  1184.069007   0.721100    2.000000    2.000000    2.000000    692.666667            NaN            NaN            NaN\n                   optics                  0.351506     4.860897   0.823056    2.000000    2.000000    2.000000    692.666667     853.666667     853.666667     853.666667\n                   store_clust            -0.462240     1.024863  13.160307  125.000000  125.000000  125.000000    692.666667            NaN            NaN            NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>si_score</th>\n      <th>ch_score</th>\n      <th>db_score</th>\n      <th>si_n_clust</th>\n      <th>ch_n_clust</th>\n      <th>db_n_clust</th>\n      <th>n_components</th>\n      <th>si_n_outliers</th>\n      <th>ch_n_outliers</th>\n      <th>db_n_outliers</th>\n    </tr>\n    <tr>\n      <th>interval</th>\n      <th>frequency</th>\n      <th>clust_func</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"18\" valign=\"top\">1 days</th>\n      <th rowspan=\"6\" valign=\"top\">12H</th>\n      <th>agglomerative_complete</th>\n      <td>0.623797</td>\n      <td>146.236750</td>\n      <td>0.420281</td>\n      <td>2.141414</td>\n      <td>4.191919</td>\n      <td>2.444444</td>\n      <td>257.797980</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.704312</td>\n      <td>22.654634</td>\n      <td>0.185554</td>\n      <td>2.373737</td>\n      <td>2.595960</td>\n      <td>2.454545</td>\n      <td>257.797980</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.396977</td>\n      <td>684.754070</td>\n      <td>0.877321</td>\n      <td>2.020202</td>\n      <td>2.282828</td>\n      <td>2.121212</td>\n      <td>257.797980</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.445740</td>\n      <td>896.806330</td>\n      <td>0.829006</td>\n      <td>2.000000</td>\n      <td>2.040404</td>\n      <td>2.040404</td>\n      <td>257.797980</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.852459</td>\n      <td>2.852459</td>\n      <td>2.852459</td>\n      <td>257.797980</td>\n      <td>826.426230</td>\n      <td>826.344262</td>\n      <td>826.426230</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.446528</td>\n      <td>0.987967</td>\n      <td>11.574773</td>\n      <td>124.101010</td>\n      <td>124.101010</td>\n      <td>124.101010</td>\n      <td>257.797980</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">1H</th>\n      <th>agglomerative_complete</th>\n      <td>0.585759</td>\n      <td>83.599234</td>\n      <td>0.370169</td>\n      <td>2.222222</td>\n      <td>5.222222</td>\n      <td>4.000000</td>\n      <td>596.808081</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.661750</td>\n      <td>17.004771</td>\n      <td>0.212829</td>\n      <td>2.080808</td>\n      <td>2.171717</td>\n      <td>2.090909</td>\n      <td>596.808081</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.413867</td>\n      <td>820.678791</td>\n      <td>0.827009</td>\n      <td>2.090909</td>\n      <td>2.606061</td>\n      <td>6.383838</td>\n      <td>596.808081</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.425849</td>\n      <td>831.248277</td>\n      <td>0.741595</td>\n      <td>2.090909</td>\n      <td>2.272727</td>\n      <td>9.666667</td>\n      <td>596.808081</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.600000</td>\n      <td>2.600000</td>\n      <td>2.600000</td>\n      <td>596.808081</td>\n      <td>828.000000</td>\n      <td>828.000000</td>\n      <td>828.000000</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.475728</td>\n      <td>0.971255</td>\n      <td>13.100335</td>\n      <td>124.101010</td>\n      <td>124.101010</td>\n      <td>124.101010</td>\n      <td>596.808081</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">24H</th>\n      <th>agglomerative_complete</th>\n      <td>0.633894</td>\n      <td>175.790316</td>\n      <td>0.427488</td>\n      <td>2.090909</td>\n      <td>3.737374</td>\n      <td>2.181818</td>\n      <td>177.787879</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.710104</td>\n      <td>23.594297</td>\n      <td>0.181980</td>\n      <td>2.454545</td>\n      <td>2.646465</td>\n      <td>2.494949</td>\n      <td>177.787879</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.398989</td>\n      <td>683.741942</td>\n      <td>0.887107</td>\n      <td>2.010101</td>\n      <td>2.252525</td>\n      <td>2.161616</td>\n      <td>177.787879</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.447155</td>\n      <td>893.275408</td>\n      <td>0.836087</td>\n      <td>2.000000</td>\n      <td>2.090909</td>\n      <td>2.030303</td>\n      <td>177.787879</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>3.615385</td>\n      <td>3.615385</td>\n      <td>3.615385</td>\n      <td>177.787879</td>\n      <td>824.961538</td>\n      <td>824.961538</td>\n      <td>824.961538</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.437231</td>\n      <td>0.990746</td>\n      <td>11.196497</td>\n      <td>124.101010</td>\n      <td>124.101010</td>\n      <td>124.101010</td>\n      <td>177.787879</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"18\" valign=\"top\">3 days</th>\n      <th rowspan=\"6\" valign=\"top\">12H</th>\n      <th>agglomerative_complete</th>\n      <td>0.635210</td>\n      <td>187.607048</td>\n      <td>0.429112</td>\n      <td>2.031250</td>\n      <td>3.031250</td>\n      <td>2.250000</td>\n      <td>462.968750</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.724681</td>\n      <td>25.663587</td>\n      <td>0.174352</td>\n      <td>2.312500</td>\n      <td>2.375000</td>\n      <td>2.343750</td>\n      <td>462.968750</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.417461</td>\n      <td>842.404190</td>\n      <td>0.815893</td>\n      <td>2.031250</td>\n      <td>2.281250</td>\n      <td>2.125000</td>\n      <td>462.968750</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.474212</td>\n      <td>1042.178723</td>\n      <td>0.774211</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.062500</td>\n      <td>462.968750</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>462.968750</td>\n      <td>842.200000</td>\n      <td>842.200000</td>\n      <td>842.200000</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.458444</td>\n      <td>0.977608</td>\n      <td>12.076956</td>\n      <td>124.375000</td>\n      <td>124.375000</td>\n      <td>124.375000</td>\n      <td>462.968750</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">1H</th>\n      <th>agglomerative_complete</th>\n      <td>0.556285</td>\n      <td>86.095992</td>\n      <td>0.399960</td>\n      <td>3.906250</td>\n      <td>5.125000</td>\n      <td>5.687500</td>\n      <td>696.562500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.667593</td>\n      <td>17.796566</td>\n      <td>0.210109</td>\n      <td>2.031250</td>\n      <td>2.125000</td>\n      <td>2.031250</td>\n      <td>696.562500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.393620</td>\n      <td>843.800244</td>\n      <td>0.840791</td>\n      <td>2.156250</td>\n      <td>2.750000</td>\n      <td>5.343750</td>\n      <td>696.562500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.441683</td>\n      <td>895.855237</td>\n      <td>0.703650</td>\n      <td>2.125000</td>\n      <td>2.250000</td>\n      <td>12.250000</td>\n      <td>696.562500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>696.562500</td>\n      <td>839.250000</td>\n      <td>839.250000</td>\n      <td>839.250000</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.474911</td>\n      <td>0.949261</td>\n      <td>13.336408</td>\n      <td>124.375000</td>\n      <td>124.375000</td>\n      <td>124.375000</td>\n      <td>696.562500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">24H</th>\n      <th>agglomerative_complete</th>\n      <td>0.645298</td>\n      <td>251.796994</td>\n      <td>0.409973</td>\n      <td>2.187500</td>\n      <td>3.625000</td>\n      <td>2.187500</td>\n      <td>338.312500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.739415</td>\n      <td>28.415144</td>\n      <td>0.164843</td>\n      <td>2.281250</td>\n      <td>2.250000</td>\n      <td>2.250000</td>\n      <td>338.312500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.444756</td>\n      <td>858.666361</td>\n      <td>0.799871</td>\n      <td>2.031250</td>\n      <td>2.343750</td>\n      <td>2.125000</td>\n      <td>338.312500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.480562</td>\n      <td>1033.821318</td>\n      <td>0.777096</td>\n      <td>2.000000</td>\n      <td>2.187500</td>\n      <td>2.031250</td>\n      <td>338.312500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>338.312500</td>\n      <td>838.125000</td>\n      <td>838.125000</td>\n      <td>838.125000</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.452745</td>\n      <td>0.982078</td>\n      <td>11.836771</td>\n      <td>124.375000</td>\n      <td>124.375000</td>\n      <td>124.375000</td>\n      <td>338.312500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"18\" valign=\"top\">7 days</th>\n      <th rowspan=\"6\" valign=\"top\">12H</th>\n      <th>agglomerative_complete</th>\n      <td>0.621630</td>\n      <td>181.953754</td>\n      <td>0.420023</td>\n      <td>2.071429</td>\n      <td>4.357143</td>\n      <td>2.714286</td>\n      <td>592.928571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.730381</td>\n      <td>26.584817</td>\n      <td>0.171315</td>\n      <td>2.142857</td>\n      <td>2.142857</td>\n      <td>2.142857</td>\n      <td>592.928571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.422760</td>\n      <td>862.347214</td>\n      <td>0.817751</td>\n      <td>2.000000</td>\n      <td>2.071429</td>\n      <td>2.071429</td>\n      <td>592.928571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.482133</td>\n      <td>1114.683170</td>\n      <td>0.750728</td>\n      <td>2.000000</td>\n      <td>2.071429</td>\n      <td>2.000000</td>\n      <td>592.928571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.333333</td>\n      <td>2.333333</td>\n      <td>2.333333</td>\n      <td>592.928571</td>\n      <td>844.000000</td>\n      <td>844.000000</td>\n      <td>844.000000</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.467556</td>\n      <td>0.963544</td>\n      <td>12.394957</td>\n      <td>124.714286</td>\n      <td>124.714286</td>\n      <td>124.714286</td>\n      <td>592.928571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">1H</th>\n      <th>agglomerative_complete</th>\n      <td>0.629730</td>\n      <td>67.303724</td>\n      <td>0.294955</td>\n      <td>2.000000</td>\n      <td>4.357143</td>\n      <td>2.357143</td>\n      <td>725.428571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.661559</td>\n      <td>17.220069</td>\n      <td>0.214293</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>725.428571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.404066</td>\n      <td>872.840890</td>\n      <td>0.819001</td>\n      <td>2.071429</td>\n      <td>2.142857</td>\n      <td>2.000000</td>\n      <td>725.428571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.457032</td>\n      <td>900.184541</td>\n      <td>0.616099</td>\n      <td>2.000000</td>\n      <td>2.142857</td>\n      <td>2.642857</td>\n      <td>725.428571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.111111</td>\n      <td>2.111111</td>\n      <td>2.111111</td>\n      <td>725.428571</td>\n      <td>841.222222</td>\n      <td>841.222222</td>\n      <td>841.222222</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.474369</td>\n      <td>0.937278</td>\n      <td>13.449668</td>\n      <td>124.714286</td>\n      <td>124.714286</td>\n      <td>124.714286</td>\n      <td>725.428571</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">24H</th>\n      <th>agglomerative_complete</th>\n      <td>0.598556</td>\n      <td>332.836766</td>\n      <td>0.512786</td>\n      <td>2.214286</td>\n      <td>3.071429</td>\n      <td>9.285714</td>\n      <td>499.857143</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.748071</td>\n      <td>29.153764</td>\n      <td>0.162045</td>\n      <td>2.071429</td>\n      <td>2.357143</td>\n      <td>2.071429</td>\n      <td>499.857143</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.449453</td>\n      <td>896.549292</td>\n      <td>0.780988</td>\n      <td>2.071429</td>\n      <td>2.357143</td>\n      <td>2.142857</td>\n      <td>499.857143</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.488117</td>\n      <td>1097.164652</td>\n      <td>0.754173</td>\n      <td>2.000000</td>\n      <td>2.142857</td>\n      <td>2.000000</td>\n      <td>499.857143</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>499.857143</td>\n      <td>843.666667</td>\n      <td>843.666667</td>\n      <td>843.666667</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.465721</td>\n      <td>0.967419</td>\n      <td>12.176733</td>\n      <td>124.714286</td>\n      <td>124.714286</td>\n      <td>124.714286</td>\n      <td>499.857143</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"18\" valign=\"top\">14 days</th>\n      <th rowspan=\"6\" valign=\"top\">12H</th>\n      <th>agglomerative_complete</th>\n      <td>0.609651</td>\n      <td>106.208618</td>\n      <td>0.466463</td>\n      <td>2.000000</td>\n      <td>3.285714</td>\n      <td>2.714286</td>\n      <td>678.571429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.733344</td>\n      <td>26.562779</td>\n      <td>0.169923</td>\n      <td>2.142857</td>\n      <td>2.142857</td>\n      <td>2.142857</td>\n      <td>678.571429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.442391</td>\n      <td>941.583276</td>\n      <td>0.777610</td>\n      <td>2.000000</td>\n      <td>2.142857</td>\n      <td>2.000000</td>\n      <td>678.571429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.488102</td>\n      <td>1166.282396</td>\n      <td>0.732030</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>678.571429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>678.571429</td>\n      <td>848.666667</td>\n      <td>848.666667</td>\n      <td>848.666667</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.479499</td>\n      <td>1.049868</td>\n      <td>12.803594</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>678.571429</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">1H</th>\n      <th>agglomerative_complete</th>\n      <td>0.569308</td>\n      <td>57.542639</td>\n      <td>0.428236</td>\n      <td>2.000000</td>\n      <td>4.714286</td>\n      <td>2.571429</td>\n      <td>744.285714</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.663442</td>\n      <td>16.481117</td>\n      <td>0.213742</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>744.285714</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.395519</td>\n      <td>893.285108</td>\n      <td>0.886096</td>\n      <td>2.142857</td>\n      <td>2.428571</td>\n      <td>2.000000</td>\n      <td>744.285714</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.444943</td>\n      <td>845.995686</td>\n      <td>0.684873</td>\n      <td>2.000000</td>\n      <td>2.285714</td>\n      <td>8.428571</td>\n      <td>744.285714</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.166667</td>\n      <td>2.166667</td>\n      <td>2.166667</td>\n      <td>744.285714</td>\n      <td>846.833333</td>\n      <td>846.833333</td>\n      <td>846.833333</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.472167</td>\n      <td>1.011976</td>\n      <td>13.690378</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>744.285714</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">24H</th>\n      <th>agglomerative_complete</th>\n      <td>0.551674</td>\n      <td>342.566766</td>\n      <td>0.595995</td>\n      <td>2.000000</td>\n      <td>2.714286</td>\n      <td>3.714286</td>\n      <td>624.714286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.732241</td>\n      <td>28.545943</td>\n      <td>0.166812</td>\n      <td>2.285714</td>\n      <td>2.285714</td>\n      <td>2.285714</td>\n      <td>624.714286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.443305</td>\n      <td>968.117513</td>\n      <td>0.770155</td>\n      <td>2.000000</td>\n      <td>2.142857</td>\n      <td>2.000000</td>\n      <td>624.714286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.498118</td>\n      <td>1154.582303</td>\n      <td>0.729925</td>\n      <td>2.000000</td>\n      <td>2.142857</td>\n      <td>2.000000</td>\n      <td>624.714286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.333333</td>\n      <td>2.333333</td>\n      <td>2.333333</td>\n      <td>624.714286</td>\n      <td>846.333333</td>\n      <td>846.333333</td>\n      <td>846.333333</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.473607</td>\n      <td>1.048759</td>\n      <td>12.697083</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>624.714286</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"18\" valign=\"top\">28 days</th>\n      <th rowspan=\"6\" valign=\"top\">12H</th>\n      <th>agglomerative_complete</th>\n      <td>0.642518</td>\n      <td>115.507702</td>\n      <td>0.431183</td>\n      <td>2.000000</td>\n      <td>2.666667</td>\n      <td>2.000000</td>\n      <td>719.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.744751</td>\n      <td>25.801052</td>\n      <td>0.164648</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>719.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.433331</td>\n      <td>973.089162</td>\n      <td>0.790731</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>719.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.485615</td>\n      <td>1177.161530</td>\n      <td>0.729206</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>719.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>-inf</td>\n      <td>-inf</td>\n      <td>inf</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>719.666667</td>\n      <td>854.000000</td>\n      <td>854.000000</td>\n      <td>854.000000</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.460550</td>\n      <td>1.012619</td>\n      <td>13.222149</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>719.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">1H</th>\n      <th>agglomerative_complete</th>\n      <td>0.561293</td>\n      <td>51.244228</td>\n      <td>0.459199</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>2.000000</td>\n      <td>756.333333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.671023</td>\n      <td>15.820627</td>\n      <td>0.209331</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>756.333333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.391923</td>\n      <td>858.314201</td>\n      <td>0.830534</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>756.333333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.502379</td>\n      <td>718.782795</td>\n      <td>0.574381</td>\n      <td>2.000000</td>\n      <td>2.666667</td>\n      <td>3.666667</td>\n      <td>756.333333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>0.411184</td>\n      <td>10.519791</td>\n      <td>0.690513</td>\n      <td>3.666667</td>\n      <td>3.666667</td>\n      <td>3.666667</td>\n      <td>756.333333</td>\n      <td>848.666667</td>\n      <td>848.666667</td>\n      <td>848.666667</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.454928</td>\n      <td>0.978977</td>\n      <td>13.803514</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>756.333333</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th rowspan=\"6\" valign=\"top\">24H</th>\n      <th>agglomerative_complete</th>\n      <td>0.517136</td>\n      <td>358.034690</td>\n      <td>0.696998</td>\n      <td>2.000000</td>\n      <td>2.666667</td>\n      <td>2.333333</td>\n      <td>692.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_single</th>\n      <td>0.738454</td>\n      <td>27.435288</td>\n      <td>0.163044</td>\n      <td>2.666667</td>\n      <td>2.333333</td>\n      <td>2.666667</td>\n      <td>692.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>agglomerative_ward</th>\n      <td>0.413781</td>\n      <td>862.135546</td>\n      <td>0.788118</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>692.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>k_means</th>\n      <td>0.502071</td>\n      <td>1184.069007</td>\n      <td>0.721100</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>692.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>optics</th>\n      <td>0.351506</td>\n      <td>4.860897</td>\n      <td>0.823056</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>692.666667</td>\n      <td>853.666667</td>\n      <td>853.666667</td>\n      <td>853.666667</td>\n    </tr>\n    <tr>\n      <th>store_clust</th>\n      <td>-0.462240</td>\n      <td>1.024863</td>\n      <td>13.160307</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>125.000000</td>\n      <td>692.666667</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "F_TETS_DF = TEST_DF.groupby(level=list(range(len(TEST_INDEX)))).mean()\n",
    "F_TETS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF[\"mean_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].mean()\n",
    "TEST_DF[\"median_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].median()\n",
    "TEST_DF[\"max_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].max()\n",
    "TEST_DF[\"min_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].min()\n",
    "TEST_DF.drop([\"cophenet_score\"], axis=1, inplace=True)\n",
    "TEST_DF = TEST_DF.loc[~TEST_DF.index.duplicated(keep=\"first\")]\n",
    "TEST_DF.index = TEST_DF.index.set_levels(TEST_DF.index.levels[TEST_DF.index.names.index(\"frequency\")].str.replace(\"H\", \"\").astype(\"int32\"), level=\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot column\n",
    "COLUMN_LABEL = \"n_pca\"\n",
    "TEST_DF = TEST_DF.sort_values(by=[COLUMN_LABEL], ascending=True)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.xticks(TEST_DF.index.get_level_values(COLUMN_LABEL).to_series())\n",
    "plt.scatter(TEST_DF.index.get_level_values(COLUMN_LABEL), TEST_DF[\"mean_cophenetic_score\"])\n",
    "plt.axhline(y=TEST_DF[\"mean_cophenetic_score\"].max(), color='r', linestyle='-')\n",
    "plt.axvline(x=TEST_DF[\"mean_cophenetic_score\"].idxmax()[TEST_DF.index.names.index(COLUMN_LABEL)], color='r', linestyle='-')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,25))\n",
    "ax = sns.heatmap(TEST_DF.sort_values(by=[\"mean_cophenetic_score\"], ascending=False), cmap=sns.cm.rocket_r, annot=True, fmt=\".4g\") #.sort_values(by=[\"mean_cophenetic_score\"], ascending=False)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.label.set_size(16)\n",
    "ax.set_title(\"Hierarchical clustering comparison\", fontsize=24)\n",
    "ax.text(0.5, -0.05, \"Cophonetic Correlation Score (higher is better)\\nSample frequency: 1 hour,    Interval length: 1 week,    Number of intervals: 8\",\n",
    "        verticalalignment=\"bottom\", horizontalalignment=\"center\",\n",
    "        transform=ax.transAxes, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_METRICS = [\"euclidean\", \"cityblock\", \"cosine\", \"correlation\"]\n",
    "LINK_METHODS = [\"ward\", \"complete\", \"average\", \"single\", \"weighted\", \"centroid\", \"median\"]\n",
    "INTERVAL = pd.Timedelta(1, unit=\"W\")\n",
    "FREQ = \"1H\"\n",
    "COPHENET_SCORES = cophenet_test(TSF, INTERVAL, FREQ, DIST_METRICS, LINK_METHODS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP_DF = pd.DataFrame(COPHENET_SCORES, columns=[\"metric\", \"method\", \"cophenet_score\"])\n",
    "COP_DF.set_index([\"metric\", \"method\"], inplace=True)\n",
    "COP_DF[\"mean_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].mean()\n",
    "COP_DF[\"median_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].median()\n",
    "COP_DF[\"max_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].max()\n",
    "COP_DF[\"min_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].min()\n",
    "COP_DF.drop([\"cophenet_score\"], axis=1, inplace=True)\n",
    "COP_DF = COP_DF.loc[~COP_DF.index.duplicated(keep='first')]\n",
    "\n",
    "plt.figure(figsize=(12,15), )\n",
    "ax = sns.heatmap(COP_DF.sort_values(by=[\"mean_cophenetic_score\"], ascending=False), cmap=sns.cm.rocket_r, annot=True, fmt=\".4g\")\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.label.set_size(16)\n",
    "ax.set_title(\"Hierarchical clustering comparison\", fontsize=24)\n",
    "ax.text(0.5, -0.05, \"Cophonetic Correlation Score (higher is better)\\nSample frequency: 1 hour,    Interval length: 1 week,    Number of intervals: 8\",\n",
    "        verticalalignment=\"bottom\", horizontalalignment=\"center\",\n",
    "        transform=ax.transAxes, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA/SVD Analysis\n",
    "Investigate optimal number of PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA/SVD Analysis\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, cophenet\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mayavi.mlab as myalab\n",
    "\n",
    "SAMPLE_FREQUENCY = \"1H0min\"\n",
    "START, END = get_intervals(TSF, pd.Timedelta(1, unit=\"D\"))[15]\n",
    "print(START, \"--\", END)\n",
    "INTERVAL = TSF[(TSF.index.get_level_values(\"date\") > START) & (TSF.index.get_level_values(\"date\") <= END)]\n",
    "DF = format_ts(INTERVAL, START, END, SAMPLE_FREQUENCY, flatten=True, normalize=False)\n",
    "\n",
    "SVD_COMPONENTS = TruncatedSVD(n_components=DF.shape[1]-1).fit(DF).explained_variance_ratio_.cumsum()\n",
    "PCA_COMPONENTS = PCA().fit(DF).explained_variance_ratio_.cumsum()\n",
    "NORM_SVD_COMPONENTS = TruncatedSVD(n_components=DF.shape[1]-1).fit(scale(DF)).explained_variance_ratio_.cumsum()\n",
    "NORM_PCA_COMPONENTS = PCA().fit(scale(DF)).explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(SVD_COMPONENTS, label=\"SVD\", linestyle=\"--\")\n",
    "#plt.show()\n",
    "plt.plot(PCA_COMPONENTS, label=\"PCA\", linestyle=\":\")\n",
    "#plt.show()\n",
    "plt.plot(NORM_SVD_COMPONENTS, label=\"SVD_NORM\", linestyle=\"--\")\n",
    "#plt.show()\n",
    "plt.plot(NORM_PCA_COMPONENTS, label=\"PCA_NORM\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE/SVD Visualization\n",
    "A visual comparison between store segmentation and auto segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "def silhouette_clusters(distance_matrix, labels, metric=\"precomputed\"):\n",
    "    sample_sil_score = silhouette_samples(distance_matrix, labels, metric=metric)\n",
    "    label_sil_score = pd.DataFrame(np.stack((labels, sample_sil_score), axis=1), columns=[\"Label\", \"Score\"])\n",
    "    label_sil_score = label_sil_score.groupby(\"Label\").mean()\n",
    "    return label_sil_score\n",
    "\n",
    "# STORE_SIL_SCORE = silhouette_clusters(squareform(DIST), STORE_LABELS)\n",
    "# STORE_SIL_SCORE.rename(index=STORE_ID_TO_NAME, inplace=True)\n",
    "# STORE_SIL_SCORE.sort_values(\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, cophenet\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mayavi.mlab as myalab\n",
    "\n",
    "SAMPLE_FREQUENCY = \"3H0min\"\n",
    "START, END = get_intervals(TSF, pd.Timedelta(7, unit=\"D\"))[0]\n",
    "print(START, \"--\", END)\n",
    "INTERVAL = TSF[(TSF.index.get_level_values(\"date\") > START) & (TSF.index.get_level_values(\"date\") <= END)]\n",
    "print(\"Raw data sparsity\", sparsity_coefficient(INTERVAL))\n",
    "N_STORES = INTERVAL.index.get_level_values(\"store_id\").nunique()\n",
    "print(\"N_STORES:\", N_STORES)\n",
    "N_DEVICES = INTERVAL.index.get_level_values(\"device_id\").nunique()\n",
    "DF = format_ts(INTERVAL, START, END, SAMPLE_FREQUENCY, flatten=True, normalize=True)\n",
    "print(\"Formatted data sparsity\", sparsity_coefficient(DF))\n",
    "STORE_COLOR_MAP_2 = DF.index.get_level_values(\"store_id\")\n",
    "\n",
    "DF = scale(DF)\n",
    "print(\"Normalized data sparsity\", sparsity_coefficient(DF))\n",
    "\n",
    "SVD_COMPONENTS = TruncatedSVD(n_components=50).fit_transform(DF)\n",
    "print(\"SVD sparsity\", sparsity_coefficient(SVD_COMPONENTS))\n",
    "#SVD_COMPONENTS -= SVD_COMPONENTS.min()\n",
    "#SVD_COMPONENTS /= SVD_COMPONENTS.max()\n",
    "\n",
    "PCA_COMPONENTS_10D = PCA(n_components=10).fit_transform(SVD_COMPONENTS)\n",
    "PCA_COMPONENTS_3D = PCA(n_components=2).fit_transform(SVD_COMPONENTS)\n",
    "PCA_COMPONENTS_3D -= PCA_COMPONENTS_3D.min()\n",
    "PCA_COMPONENTS_3D /= PCA_COMPONENTS_3D.max()\n",
    "TSNE_3D = TSNE(n_components=2, perplexity=10, learning_rate=200, n_iter=10000, n_iter_without_progress=500, verbose=2, n_jobs=-1).fit_transform(PCA_COMPONENTS_10D)\n",
    "TSNE_3D -= TSNE_3D.min()\n",
    "TSNE_3D /= TSNE_3D.max()\n",
    "\n",
    "NUMBER_OF_COLORS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics(TSNE_3D, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIST = pdist(TSNE_3D, metric=\"euclidean\")\n",
    "LINK = linkage(DIST, method=\"ward\")\n",
    "CLUST = fcluster(LINK, t=125, criterion=\"maxclust\").astype(\"float64\")\n",
    "CLUST = optics(TSNE_3D, 5)\n",
    "# NUMBER_OF_COLORS = 21\n",
    "# START_CLUST = 0\n",
    "# CLUST_COLOR_MAP = np.copy(CLUST)\n",
    "# CLUST_COLOR_MAP[(CLUST_COLOR_MAP < START_CLUST) | (CLUST_COLOR_MAP > START_CLUST+NUMBER_OF_COLORS-1)] = 0.0\n",
    "# CLUST_COLOR_MAP[CLUST_COLOR_MAP > 0] -= (START_CLUST-1)\n",
    "# CLUST_COLOR_MAP -= CLUST_COLOR_MAP.min()\n",
    "# CLUST_COLOR_MAP /= CLUST_COLOR_MAP.max()\n",
    "\n",
    "STORE_SIL_SCORE = silhouette_clusters(squareform(DIST), CLUST).mean()\n",
    "print(STORE_SIL_SCORE)\n",
    "\n",
    "CLUST_COLOR_MAP = pd.DataFrame(np.copy(CLUST))\n",
    "CLUST_COLOR_MAP[~(CLUST_COLOR_MAP.isin(range(1,20)))] = 0.0\n",
    "CLUST_COLOR_MAP = CLUST_COLOR_MAP.values.T[0]\n",
    "CLUST_COLOR_MAP -= CLUST_COLOR_MAP.min()\n",
    "CLUST_COLOR_MAP /= CLUST_COLOR_MAP.max()\n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.scatter(TSNE_3D.T[0].T, TSNE_3D.T[1].T, c=CLUST_COLOR_MAP, cmap=\"tab20\")\n",
    "plt.colorbar(orientation=\"vertical\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Clusters \"+\"n=\"+str(125)+\" (showing 19)\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORE_LABELS = INTERVAL.index[~INTERVAL.index.get_level_values(\"device_id\").duplicated()].get_level_values(\"store_id\").astype(\"float64\").values\n",
    "# START_COLOR = 50\n",
    "# STORE_COLOR_MAP = np.copy(STORE_LABELS)\n",
    "# STORE_COLOR_MAP[(STORE_COLOR_MAP < START_COLOR) | (STORE_COLOR_MAP > START_COLOR+NUMBER_OF_COLORS-1)] = 0.0\n",
    "# STORE_COLOR_MAP[STORE_COLOR_MAP > 0] -= (START_COLOR-1)\n",
    "# STORE_COLOR_MAP -= STORE_COLOR_MAP.min()\n",
    "# STORE_COLOR_MAP /= STORE_COLOR_MAP.max()\n",
    "\n",
    "STORE_SIL_SCORE = silhouette_clusters(squareform(DIST), STORE_LABELS)\n",
    "STORE_SIL_SCORE.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "TOP_DICT = {old:(new+1) for new, old in enumerate(STORE_SIL_SCORE.iloc[np.r_[0:4, -4:-0]].index.values)}\n",
    "STORE_COLOR_MAP = pd.DataFrame(np.copy(STORE_LABELS))\n",
    "\n",
    "STORE_COLOR_MAP[~(STORE_COLOR_MAP.isin(TOP_DICT.keys()))] = 0.0\n",
    "STORE_COLOR_MAP.replace({0: TOP_DICT}, inplace=True)\n",
    "STORE_COLOR_MAP = STORE_COLOR_MAP.values.T[0]\n",
    "STORE_COLOR_MAP -= STORE_COLOR_MAP.min()\n",
    "STORE_COLOR_MAP /= STORE_COLOR_MAP.max()\n",
    "\n",
    "STORE_SIL_SCORE.rename(index=STORE_ID_TO_NAME, inplace=True)\n",
    "print(STORE_SIL_SCORE.mean())\n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.scatter(TSNE_3D.T[0].T, TSNE_3D.T[1].T, c=STORE_COLOR_MAP, cmap=\"Set1_r\")\n",
    "plt.colorbar(orientation=\"vertical\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Stores \"+\"n=\"+str(N_STORES)+\" (showing 8)\", size=20)\n",
    "plt.show()\n",
    "\n",
    "STORE_SIL_SCORE.index.set_names(['Store'], inplace=True)\n",
    "\n",
    "ax = sns.heatmap(STORE_SIL_SCORE.iloc[np.r_[0:4, -4:-0]], cmap=sns.cm.rocket_r, annot=True, fmt=\".4g\")\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.label.set_size(16)\n",
    "ax.set_title(\"Store Silhouette Score\", fontsize=24)\n",
    "#ax.text(0.5, -0.05, \"Cophonetic Correlation Score (higher is better)\\nSample frequency: 1 hour,    Interval length: 1 week,    Number of intervals: 8\", verticalalignment=\"bottom\", horizontalalignment=\"center\", transform=ax.transAxes, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_DIST = pdist(TSNE_3D, metric=\"euclidean\")\n",
    "STORE_SIL_SCORE = silhouette_clusters(squareform(STORE_DIST), STORE_LABELS)\n",
    "#STORE_SIL_SCORE.rename(index=STORE_ID_TO_NAME, inplace=True)\n",
    "STORE_SIL_SCORE.sort_values(\"Score\", ascending=False).iloc[np.r_[:5,-5:0]].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST = pdist(SVD_COMPONENTS, metric=\"euclidean\")\n",
    "LINK = linkage(DIST, method=\"complete\")\n",
    "CLUST = fcluster(LINK, t=N_STORES, criterion=\"maxclust\").astype(\"float64\")\n",
    "START_CLUST = 0\n",
    "CLUST_COLOR_MAP = np.copy(CLUST)\n",
    "CLUST_COLOR_MAP[(CLUST_COLOR_MAP < START_CLUST) | (CLUST_COLOR_MAP > START_CLUST+NUMBER_OF_COLORS-1)] = 0.0\n",
    "CLUST_COLOR_MAP[CLUST_COLOR_MAP > 0] -= (START_CLUST-1)\n",
    "CLUST_COLOR_MAP -= CLUST_COLOR_MAP.min()\n",
    "CLUST_COLOR_MAP /= CLUST_COLOR_MAP.max()\n",
    "\n",
    "#nodes = myalab.points3d(TSNE_3D.T[0].T, TSNE_3D.T[1].T, TSNE_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes = myalab.points3d(PCA_COMPONENTS_3D.T[0].T, PCA_COMPONENTS_3D.T[1].T, PCA_COMPONENTS_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes.glyph.scale_mode = 'scale_by_vector'\n",
    "nodes.mlab_source.dataset.point_data.scalars = list(CLUST_COLOR_MAP)\n",
    "myalab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORE_LABELS = INTERVAL.index[~INTERVAL.index.get_level_values(\"device_id\").duplicated()].get_level_values(\"store_id\").astype(\"float64\").values\n",
    "START_COLOR = 50\n",
    "STORE_COLOR_MAP = np.copy(STORE_LABELS)\n",
    "STORE_COLOR_MAP[(STORE_COLOR_MAP < START_COLOR) | (STORE_COLOR_MAP > START_COLOR+NUMBER_OF_COLORS-1)] = 0.0\n",
    "STORE_COLOR_MAP[STORE_COLOR_MAP > 0] -= (START_COLOR-1)\n",
    "STORE_COLOR_MAP -= STORE_COLOR_MAP.min()\n",
    "STORE_COLOR_MAP /= STORE_COLOR_MAP.max()\n",
    "\n",
    "#nodes = myalab.points3d(TSNE_3D.T[0].T, TSNE_3D.T[1].T, TSNE_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes = myalab.points3d(PCA_COMPONENTS_3D.T[0].T, PCA_COMPONENTS_3D.T[1].T, PCA_COMPONENTS_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes.glyph.scale_mode = 'scale_by_vector'\n",
    "nodes.mlab_source.dataset.point_data.scalars = list(STORE_COLOR_MAP)\n",
    "myalab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Silhoutte score explained\n",
    "def gauss_2d(center, sigma, n_points):\n",
    "    return np.random.multivariate_normal(center, ((sigma,0), (0, sigma)), n_points).T\n",
    "\n",
    "def plot_gauss_clusters(clusters, axis_limits, title=\"\"):\n",
    "    markers = [\"o\", \"v\", \"s\", \"^\"]\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.title(title)\n",
    "    plt.xlim(*axis_limits)\n",
    "    plt.ylim(*axis_limits)\n",
    "    for i, (x, y) in enumerate(clusters):\n",
    "        plt.scatter(x, y, label=\"Cluster \"+str(i+1), marker=markers[i%4])\n",
    "    plt.legend()\n",
    "    plt.axis('off')        \n",
    "    plt.show()\n",
    "\n",
    "def synth_silhouette(clusters):\n",
    "    arr = np.array([(i, x, y) for i, cluster in enumerate(clusters) for x, y in cluster.T])\n",
    "    df =  pd.DataFrame(arr, columns=[\"label\", \"x\", \"y\"])\n",
    "    dist = pdist(df[[\"x\", \"y\"]], metric=\"euclidean\")\n",
    "    return silhouette_score(squareform(dist), df[\"label\"])\n",
    "\n",
    "CLUSTERS = [gauss_2d((5.5,5.5), 0.02, 80), gauss_2d((5,4.5), 0.02, 80), gauss_2d((4.5,5.5), 0.02, 80)]\n",
    "plot_gauss_clusters(CLUSTERS, (4,6), \"Possitive Silhoutte Score\")\n",
    "print(synth_silhouette(CLUSTERS))\n",
    "\n",
    "CLUSTERS = [gauss_2d((5.1,5.1), 0.3, 80), gauss_2d((5,4.6), 0.3, 80), gauss_2d((4.6,5.1), 0.3, 80)]\n",
    "plot_gauss_clusters(CLUSTERS, (3,7), \"Zero Silhoutte Score\")\n",
    "print(synth_silhouette(CLUSTERS))\n",
    "\n",
    "CLUST_1 = np.concatenate([gauss_2d((5.5,5.5), 0.02, 20), gauss_2d((5,4.5), 0.02, 20), gauss_2d((4.5,5.5), 0.02, 20)], axis=1)\n",
    "CLUST_2 = np.concatenate([gauss_2d((5.5,5.5), 0.02, 20), gauss_2d((5,4.5), 0.02, 20), gauss_2d((4.5,5.5), 0.02, 20)], axis=1)\n",
    "CLUST_3 = np.concatenate([gauss_2d((5.5,5.5), 0.02, 20), gauss_2d((5,4.5), 0.02, 20), gauss_2d((4.5,5.5), 0.02, 20)], axis=1)\n",
    "CLUSTERS = [CLUST_1, CLUST_2, CLUST_3]\n",
    "plot_gauss_clusters(CLUSTERS, (4,6), \"Negative Silhoutte Score\")\n",
    "print(synth_silhouette(CLUSTERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "#Example plot of kiosk MVTS\n",
    "SAMPLE_FREQUENCY = \"1H0min\"\n",
    "START, END = get_intervals(TSF, pd.Timedelta(2, unit=\"D\"))[2]\n",
    "print(START, \"--\", END)\n",
    "INTERVAL = TSF[(TSF.index.get_level_values(\"date\") > START) & (TSF.index.get_level_values(\"date\") <= END)]\n",
    "DF = format_ts(INTERVAL, START, END, SAMPLE_FREQUENCY, flatten=False, normalize=True)\n",
    "DF = DF.xs(\"AwfulPerseveringIncome\", level=\"device_id\")\n",
    "TIME_VAL = list(DF.index.get_level_values(\"date\"))\n",
    "plt.figure(figsize=(14,6))\n",
    "ax = plt.gca()\n",
    "yearFmt = mdates.DateFormatter(\"%H:%M\")\n",
    "\n",
    "DF.unstack(level=0).plot(y=[\"product_256\"], ax=ax, label=[\"Kids meal\"], linestyle=\"--\")\n",
    "DF.unstack(level=0).plot(y=[\"product_144\"], ax=ax, label=[\"Chili cheese\"], linestyle=\"-.\")\n",
    "DF.unstack(level=0).plot(y=[\"product_394\"], ax=ax, label=[\"Smokey Chipotle Chicken Menu\"], linestyle=\":\")\n",
    "DF.unstack(level=0).plot(y=[\"average_price\"], ax=ax, label=[\"Average Purchase Value\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Values (0,1)\")\n",
    "ax.xaxis.set_major_formatter(yearFmt)\n",
    "xticks = ax.xaxis.get_major_ticks()\n",
    "xticks[0].label1.set_visible(False)\n",
    "xticks[-1].label1.set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "ax = plt.gca()\n",
    "DF.unstack(level=0).plot(ax=ax)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Values (0,1)\")\n",
    "ax.xaxis.set_major_formatter(yearFmt)\n",
    "xticks = ax.xaxis.get_major_ticks()\n",
    "xticks[0].label1.set_visible(False)\n",
    "xticks[-1].label1.set_visible(False)\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_cols(df, row_index, n_cols):\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    row_frame = df.iloc[row_index].copy()\n",
    "    columns = []\n",
    "    for _ in range(n_cols):\n",
    "        if len(row_frame) == 0:\n",
    "            break\n",
    "        col_id = row_frame.idxmax()\n",
    "        columns.append(col_id)\n",
    "        row_frame = row_frame.drop([col_id])\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_dict(labels):\n",
    "    #labels = [col for col in df.columns if col not in [\"device_id\", \"date\", \"store_id\", \"class\", \"total_price\"]]\n",
    "    colors = [cm.rainbow(i) for i in np.linspace(0, 1, len(labels))]\n",
    "    c_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        c_dict[label] = colors[i]\n",
    "    return c_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START_INDEX = 0\n",
    "N_COLS = 6\n",
    "COL_SET = set()\n",
    "for D in DEVICES:\n",
    "    DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == D[0], COL_WL].copy()\n",
    "    DEVICE_FRAME = DEVICE_FRAME.drop([\"total_price\"], axis=1)\n",
    "    DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").sum().iloc[START_INDEX:]\n",
    "    DEVICE_FRAME = DEVICE_FRAME.cumsum()\n",
    "    for C in get_top_cols(DEVICE_FRAME, -1, N_COLS):\n",
    "        COL_SET.add(C)\n",
    "COLOR_DICT = get_color_dict(COL_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for D in DEVICES:\n",
    "    DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == D[0], column_whitelist].copy()\n",
    "    if len(DEVICE_FRAME) == 0:\n",
    "        continue\n",
    "    DEVICE_FRAME = DEVICE_FRAME.drop([\"total_price\"], axis=1)\n",
    "    DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").sum().iloc[START_INDEX:]\n",
    "    DEVICE_FRAME = DEVICE_FRAME.cumsum()\n",
    "    COLS = get_top_cols(DEVICE_FRAME, -1, N_COLS)\n",
    "    fig = plt.figure()\n",
    "    DEVICE_FRAME.plot(kind='line',y=COLS, figsize=(16, 10), color=[COLOR_DICT.get(x, '#666666') for x in COLS], linewidth=3.0)\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.figtext(.5,.9, \"Device \"+str(D[0])+\", Store \"+str(D[2]),fontsize=24)\n",
    "    #plt.show()\n",
    "    plt.savefig(\"C:/Users/user/Desktop/store_/\"+str(D[2]+\"/\"+\"device_\"+str(D[0])+\"_day.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_cols(df):\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"day_of_week\"] = df.index.dayofweek\n",
    "    df[\"day_of_month\"] = df.index.day\n",
    "    df[\"month\"] = df.index.month\n",
    "    holidays_swe = holidays.Sweden(include_sundays=False)[df.index[0]: df.index[-1]]\n",
    "    df[\"holiday\"] = [1 if d in holidays_swe else 0 for d in df.index.date]\n",
    "\n",
    "def remove_zero_sequence(df, col, min_length):\n",
    "    mask = col.groupby((col != col.shift()).cumsum()).transform('count').lt(min_length)\n",
    "    mask = ~(mask | col.gt(0))\n",
    "    df.drop(mask[mask].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_split(df, train_len=pd.Timedelta(4, unit='W'), forecast_len=pd.Timedelta(1, unit='W'), expanding_window=False):\n",
    "    start = df.index.values[0]\n",
    "    split = start + train_len\n",
    "    forecast_end = split + forecast_len\n",
    "    end = df.index.values[-1]\n",
    "    sets = []\n",
    "    while end > forecast_end:\n",
    "        sets.append((df[start:split].index, df[split:forecast_end].index))\n",
    "        if not expanding_window:\n",
    "            start += train_len + forecast_len\n",
    "        split += train_len + forecast_len\n",
    "        forecast_end += train_len + forecast_len\n",
    "    return sets\n",
    "\n",
    "DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == DEVICES[0][0], COL_WL].copy()\n",
    "len(get_test_train_split(DEVICE_FRAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "def train_and_predict_var(train_set, n_steps):\n",
    "    model = VAR(endog=train_set)\n",
    "    model_fit = model.fit(maxlags=2, trend=\"nc\")\n",
    "    print(model_fit.y)\n",
    "    return\n",
    "    prediction = model_fit.forecast(model_fit.y, steps=n_steps)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "def validate(predictions, true_val):\n",
    "    sum_meanAE = mean_absolute_error(predictions, true_val).sum()\n",
    "    sum_medianAE = median_absolute_error(predictions, true_val).sum()\n",
    "    return sum_meanAE, sum_medianAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == DEVICES[0][0], COL_WL].copy()\n",
    "#DEVICE_FRAME[\"orders\"] = 1\n",
    "AGG_DICT = {col_name:np.sum for col_name in DEVICE_FRAME.columns}\n",
    "AGG_DICT.update({\"total_price\":np.mean})\n",
    "DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").agg(AGG_DICT).fillna(0).cumsum()\n",
    "DEVICE_FRAME.rename(columns={'total_price': 'average_price'}, inplace=True)\n",
    "DEVICE_FRAME.drop([\"average_price\"], axis=1, inplace=True)\n",
    "#remove_zero_sequence(DEVICE_FRAME, DEVICE_FRAME.orders, 25)\n",
    "#add_time_cols(DEVICE_FRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_train = get_test_train_split(DEVICE_FRAME)\n",
    "pred = train_and_predict_var(DEVICE_FRAME.loc[test_train[0][0]], len(test_train[0][1]))\n",
    "validate(pred, DEVICE_FRAME.loc[test_train[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUM_TRUE = DEVICE_FRAME.loc[test_train[0][1]]#DEVICE_FRAME.loc[test_train[0][0]].append(DEVICE_FRAME.loc[test_train[0][1]])\n",
    "PRED = pd.DataFrame(index=SUM_TRUE.index,columns=SUM_TRUE.columns)\n",
    "for j in range(0,len(DEVICE_FRAME.loc[test_train[0][1]].columns)):\n",
    "    for i in range(0, len(pred)):\n",
    "       PRED.iloc[i][j] = pred[i][j]\n",
    "SUM_VAR = PRED\n",
    "COLS = get_top_cols(SUM_TRUE, -1, 5)\n",
    "COLOR_DICT = get_color_dict(COLS)\n",
    "_, ax = plt.subplots()\n",
    "SUM_TRUE.plot(ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "SUM_VAR.plot(linestyle='dashed', ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "plt.legend(prop={'size': 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in get_test_train_split(DEVICE_FRAME, pd.Timedelta(4, unit='W'), pd.Timedelta(1, unit='W')):\n",
    "    train = DEVICE_FRAME.loc[train]\n",
    "    test = DEVICE_FRAME.loc[test]#-train.iloc[-1]\n",
    "    pred = train_and_predict_var(train, len(test))\n",
    "    #validate(pred, test)\n",
    "\n",
    "    SUM_TRUE = test#DEVICE_FRAME.loc[test_train[0][0]].append(DEVICE_FRAME.loc[test_train[0][1]])\n",
    "    PRED = pd.DataFrame(index=test.index,columns=test.columns)\n",
    "    for j in range(0,len(test.columns)):\n",
    "        for i in range(0, len(pred)):\n",
    "            PRED.iloc[i][j] = pred[i][j]\n",
    "    SUM_VAR = PRED#-train.iloc[-1]\n",
    "    COLS = get_top_cols(SUM_TRUE, -1, 5)\n",
    "    COLOR_DICT = get_color_dict(COLS)\n",
    "    _, ax = plt.subplots()\n",
    "    train.append(SUM_TRUE).plot(ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "    SUM_VAR.plot(linestyle='dashed', ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.scatter(DEVICE_FRAME.index, DEVICE_FRAME[\"orders\"].cumsum())\n",
    "plt.figure(figsize=(20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "TRAIN_SIZE = -336#int(len(DEVICE_FRAME)*TRAIN_PERCENT)\n",
    "TRAIN_Y = DEVICE_FRAME[:TRAIN_SIZE]\n",
    "TRAIN_Y = TRAIN_Y.loc[:, (TRAIN_Y != TRAIN_Y.iloc[0]).any()]\n",
    "VAL_Y = DEVICE_FRAME[TRAIN_Y.columns][TRAIN_SIZE:]\n",
    "#TRAIN_X = DEVICE_FRAME[\"day\"]\n",
    "#VAL_X = DEVICE_FRAME[\"day\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creating the train and validation set\n",
    "train = TRAIN_Y\n",
    "valid = VAL_Y\n",
    "naive_pred = DEVICE_FRAME[TRAIN_SIZE-1:-1]\n",
    "\n",
    "#fit the model\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "model = VAR(endog=train)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction on validation\n",
    "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=valid.index,columns=VAL_Y.columns)\n",
    "for j in range(0,len(VAL_Y.columns)):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "#check rmse\n",
    "print(\"MAE VAR, MAE Naive, Min Val, Max Val\")\n",
    "MIN = valid.min()\n",
    "MAX = valid.max()\n",
    "SUM_MAE = 0\n",
    "for i in pred.columns:\n",
    "    SUM_MAE += mean_absolute_error(pred[i], valid[i])\n",
    "    print('MAE value for', str(i)+':\\t', mean_absolute_error(pred[i], valid[i]), \"\\t\", mean_absolute_error(naive_pred[i], valid[i]), \"\\t\", MIN[i], \"\\t\", MAX[i])\n",
    "print(\"AVG MAE:\", (SUM_MAE/len(VAL_Y.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=valid.index,columns=DEVICE_FRAME.columns)\n",
    "for j in range(0,len(DEVICE_FRAME.columns)):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "#check rmse\n",
    "print(\"RMSE for VAR predictions and Naive Predictions\")\n",
    "for i in DEVICE_FRAME.columns:\n",
    "    print('rmse value for', i, 'is : ', sqrt(mean_squared_error(pred[i], valid[i])), \"\\t\", sqrt(mean_squared_error(naive_pred[i], valid[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "DEVICE_FRAME = sc.fit_transform(DEVICE_FRAME)\n",
    "DEVICE_FRAME.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}