{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib import cm\n",
    "import json\n",
    "from itertools import zip_longest\n",
    "import os\n",
    "from math import sqrt\n",
    "import holidays\n",
    "\n",
    "# instance_varibles\n",
    "# GLOBAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../veritas_data/parser_menus/menu.json\", \"r\") as f:\n",
    "    MENU = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ts_data(file_name):\n",
    "    print(\"Loading main columns...\")\n",
    "    df = pd.read_csv(file_name, usecols=[\"device_id\", \"date\", \"store_id\", \"class\", \"total_price\"], header=0)\n",
    "\n",
    "    print(\"Formatting main columns...\")\n",
    "    df[\"device_id\"], device_map = df[\"device_id\"].factorize()\n",
    "    df[\"store_id\"], store_map = df[\"store_id\"].factorize()\n",
    "    df[\"class\"], class_map = df[\"class\"].factorize()\n",
    "    df = df.astype(\"uint32\")\n",
    "\n",
    "    print(\"Loading products...\")\n",
    "    with open(file_name, \"r\") as f:\n",
    "        num_columns = len(f.readline().split(\",\"))\n",
    "    for cols in zip_longest(*(iter(range(5, num_columns)),) * 50):\n",
    "        cols = [c for c in cols if c is not None]\n",
    "        temp_df = pd.read_csv(file_name, usecols=list(cols), header=0, dtype=\"uint8\")\n",
    "        temp_df = temp_df.loc[:, (temp_df != 0).any(axis=0)]   # Remove \"zero\" columns \n",
    "        df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "    wl = [col for col in df.columns if col not in ['device_id', 'store_id', 'class', \"date\"]]\n",
    "    df = df.drop(df[df[wl].eq(0).all(axis=1)].index)\n",
    "    df['date'] = pd.to_datetime(df['date'], unit='s')\n",
    "    df.index = df[\"date\"]\n",
    "    df = df.drop([\"date\"], axis=1)\n",
    "    return df, device_map, store_map, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_FILE = \"../veritas_data/post_parser_orders/device_time_series.csv\"\n",
    "TSF, DEVICE_MAP, STORE_MAP, CLASS_MAP = load_ts_data(TS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Returns the device_id, number of orders and the data frame of the device with the most orders\n",
    "def get_top_devices(device_id_list, ts_frame, n):\n",
    "    top_devices = []\n",
    "    for device_id in device_id_list:\n",
    "        df = ts_frame.loc[ts_frame[\"device_id\"] == device_id, [\"store_id\"]]\n",
    "        try:\n",
    "            store_id = df.values[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        num_orders = len(df.index)\n",
    "        top_devices.append((device_id, num_orders, store_id))\n",
    "        top_devices = sorted(top_devices, key=lambda d: d[1], reverse=True)[:n]\n",
    "    return top_devices\n",
    "\n",
    "# Returns df without rows where col_name equals a value occuring less than threshold times\n",
    "def trim_low_occurance_values(df, col_name, threshold):\n",
    "    s = df[col_name].value_counts().ge(threshold)\n",
    "    return df[df[col_name].isin(s[s].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FREQUENCY = \"1H\"\n",
    "COL_WL = [col for col in TSF.columns if col not in ['device_id', 'store_id', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# \n",
    "def format_ts(df, start, end, sample_freq):\n",
    "    #df[\"orders\"] = 1\n",
    "    agg_dict = {col_name:np.sum for col_name in df.columns}\n",
    "    agg_dict.update({\"total_price\":np.mean})\n",
    "    df = df.resample(sample_freq).agg(agg_dict).fillna(0).cumsum()\n",
    "    df.rename(columns={'total_price': 'average_price'}, inplace=True)\n",
    "    df.drop([\"average_price\"], axis=1, inplace=True)\n",
    "    new_df = pd.DataFrame(index=pd.date_range(start, end, freq=sample_freq), columns=df.columns)\n",
    "    new_df.update(df)\n",
    "    new_df = new_df.fillna(0)\n",
    "    #remove_zero_sequence(DEVICE_FRAME, DEVICE_FRAME.orders, 25)\n",
    "    #add_time_cols(DEVICE_FRAME)\n",
    "    return new_df\n",
    "\n",
    "# \n",
    "def get_intervals(df, t_delta, sample_freq):\n",
    "    first_date = df.index.min().floor(sample_freq)\n",
    "    last_date = (df.index.max() + pd.Timedelta(sample_freq)).floor(sample_freq)\n",
    "    time_step = last_date\n",
    "    intervals = []\n",
    "    while time_step > first_date:\n",
    "        start = time_step-t_delta\n",
    "        end = time_step\n",
    "        intervals.append((start, end))\n",
    "        time_step -= t_delta\n",
    "    return intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START, END = get_intervals(TSF, pd.Timedelta(1, unit=\"W\"), SAMPLE_FREQUENCY)[0]\n",
    "DEVICE1 = TSF[(TSF.index > START) & (TSF.index <= END)].groupby([\"device_id\"]).get_group(0)\n",
    "DEVICE1 = format_ts(DEVICE1, START, END, SAMPLE_FREQUENCY)\n",
    "DEVICE2 = TSF[(TSF.index > START) & (TSF.index <= END)].groupby([\"device_id\"]).get_group(1)\n",
    "DEVICE2 = format_ts(DEVICE2, START, END, SAMPLE_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "def error_func(df1, df2):\n",
    "    tot = 0\n",
    "    for col in df1.columns:\n",
    "        tot += np.sqrt(mean_squared_error(df1[col], df2[col]))\n",
    "    return tot, np.sqrt(mean_squared_error(df1, df2))\n",
    "\n",
    "GROUPED = TSF[(TSF.index > START) & (TSF.index <= END)].groupby([\"device_id\"])\n",
    "matrix1 = np.zeros((len(GROUPED),len(GROUPED)))\n",
    "matrix2 = np.zeros((len(GROUPED),len(GROUPED)))\n",
    "for I, (D_ID, G) in enumerate(GROUPED):\n",
    "    for I2, (D_ID2, G2) in enumerate(GROUPED):\n",
    "        matrix1[I, I2] = error_func(G, G2)[0]\n",
    "        matrix2[I, I2] = error_func(G, G2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICES = get_top_devices(range(len(DEVICE_MAP)), trim_low_occurance_values(TSF, \"device_id\", 1000), 10)\n",
    "print(DEVICE_MAP[DEVICES[0][0]], DEVICES[0][0], DEVICES[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_cols(df, row_index, n_cols):\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    row_frame = df.iloc[row_index].copy()\n",
    "    columns = []\n",
    "    for _ in range(n_cols):\n",
    "        if len(row_frame) == 0:\n",
    "            break\n",
    "        col_id = row_frame.idxmax()\n",
    "        columns.append(col_id)\n",
    "        row_frame = row_frame.drop([col_id])\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_dict(labels):\n",
    "    #labels = [col for col in df.columns if col not in [\"device_id\", \"date\", \"store_id\", \"class\", \"total_price\"]]\n",
    "    colors = [cm.rainbow(i) for i in np.linspace(0, 1, len(labels))]\n",
    "    c_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        c_dict[label] = colors[i]\n",
    "    return c_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START_INDEX = 0\n",
    "N_COLS = 6\n",
    "COL_SET = set()\n",
    "for D in DEVICES:\n",
    "    DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == D[0], COL_WL].copy()\n",
    "    DEVICE_FRAME = DEVICE_FRAME.drop([\"total_price\"], axis=1)\n",
    "    DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").sum().iloc[START_INDEX:]\n",
    "    DEVICE_FRAME = DEVICE_FRAME.cumsum()\n",
    "    for C in get_top_cols(DEVICE_FRAME, -1, N_COLS):\n",
    "        COL_SET.add(C)\n",
    "COLOR_DICT = get_color_dict(COL_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for D in DEVICES:\n",
    "    DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == D[0], column_whitelist].copy()\n",
    "    if len(DEVICE_FRAME) == 0:\n",
    "        continue\n",
    "    DEVICE_FRAME = DEVICE_FRAME.drop([\"total_price\"], axis=1)\n",
    "    DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").sum().iloc[START_INDEX:]\n",
    "    DEVICE_FRAME = DEVICE_FRAME.cumsum()\n",
    "    COLS = get_top_cols(DEVICE_FRAME, -1, N_COLS)\n",
    "    fig = plt.figure()\n",
    "    DEVICE_FRAME.plot(kind='line',y=COLS, figsize=(16, 10), color=[COLOR_DICT.get(x, '#666666') for x in COLS], linewidth=3.0)\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.figtext(.5,.9, \"Device \"+str(D[0])+\", Store \"+str(D[2]),fontsize=24)\n",
    "    #plt.show()\n",
    "    plt.savefig(\"C:/Users/user/Desktop/store_/\"+str(D[2]+\"/\"+\"device_\"+str(D[0])+\"_day.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_cols(df):\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"day_of_week\"] = df.index.dayofweek\n",
    "    df[\"day_of_month\"] = df.index.day\n",
    "    df[\"month\"] = df.index.month\n",
    "    holidays_swe = holidays.Sweden(include_sundays=False)[df.index[0]: df.index[-1]]\n",
    "    df[\"holiday\"] = [1 if d in holidays_swe else 0 for d in df.index.date]\n",
    "\n",
    "def remove_zero_sequence(df, col, min_length):\n",
    "    mask = col.groupby((col != col.shift()).cumsum()).transform('count').lt(min_length)\n",
    "    mask = ~(mask | col.gt(0))\n",
    "    df.drop(mask[mask].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_split(df, train_len=pd.Timedelta(4, unit='W'), forecast_len=pd.Timedelta(1, unit='W'), expanding_window=False):\n",
    "    start = df.index.values[0]\n",
    "    split = start + train_len\n",
    "    forecast_end = split + forecast_len\n",
    "    end = df.index.values[-1]\n",
    "    sets = []\n",
    "    while end > forecast_end:\n",
    "        sets.append((df[start:split].index, df[split:forecast_end].index))\n",
    "        if not expanding_window:\n",
    "            start += train_len + forecast_len\n",
    "        split += train_len + forecast_len\n",
    "        forecast_end += train_len + forecast_len\n",
    "    return sets\n",
    "\n",
    "DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == DEVICES[0][0], COL_WL].copy()\n",
    "len(get_test_train_split(DEVICE_FRAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "def train_and_predict_var(train_set, n_steps):\n",
    "    model = VAR(endog=train_set)\n",
    "    model_fit = model.fit(maxlags=2, trend=\"nc\")\n",
    "    print(model_fit.y)\n",
    "    return\n",
    "    prediction = model_fit.forecast(model_fit.y, steps=n_steps)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "def validate(predictions, true_val):\n",
    "    sum_meanAE = mean_absolute_error(predictions, true_val).sum()\n",
    "    sum_medianAE = median_absolute_error(predictions, true_val).sum()\n",
    "    return sum_meanAE, sum_medianAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == DEVICES[0][0], COL_WL].copy()\n",
    "#DEVICE_FRAME[\"orders\"] = 1\n",
    "AGG_DICT = {col_name:np.sum for col_name in DEVICE_FRAME.columns}\n",
    "AGG_DICT.update({\"total_price\":np.mean})\n",
    "DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").agg(AGG_DICT).fillna(0).cumsum()\n",
    "DEVICE_FRAME.rename(columns={'total_price': 'average_price'}, inplace=True)\n",
    "DEVICE_FRAME.drop([\"average_price\"], axis=1, inplace=True)\n",
    "#remove_zero_sequence(DEVICE_FRAME, DEVICE_FRAME.orders, 25)\n",
    "#add_time_cols(DEVICE_FRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_train = get_test_train_split(DEVICE_FRAME)\n",
    "pred = train_and_predict_var(DEVICE_FRAME.loc[test_train[0][0]], len(test_train[0][1]))\n",
    "validate(pred, DEVICE_FRAME.loc[test_train[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUM_TRUE = DEVICE_FRAME.loc[test_train[0][1]]#DEVICE_FRAME.loc[test_train[0][0]].append(DEVICE_FRAME.loc[test_train[0][1]])\n",
    "PRED = pd.DataFrame(index=SUM_TRUE.index,columns=SUM_TRUE.columns)\n",
    "for j in range(0,len(DEVICE_FRAME.loc[test_train[0][1]].columns)):\n",
    "    for i in range(0, len(pred)):\n",
    "       PRED.iloc[i][j] = pred[i][j]\n",
    "SUM_VAR = PRED\n",
    "COLS = get_top_cols(SUM_TRUE, -1, 5)\n",
    "COLOR_DICT = get_color_dict(COLS)\n",
    "_, ax = plt.subplots()\n",
    "SUM_TRUE.plot(ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "SUM_VAR.plot(linestyle='dashed', ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "plt.legend(prop={'size': 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in get_test_train_split(DEVICE_FRAME, pd.Timedelta(4, unit='W'), pd.Timedelta(1, unit='W')):\n",
    "    train = DEVICE_FRAME.loc[train]\n",
    "    test = DEVICE_FRAME.loc[test]#-train.iloc[-1]\n",
    "    pred = train_and_predict_var(train, len(test))\n",
    "    #validate(pred, test)\n",
    "\n",
    "    SUM_TRUE = test#DEVICE_FRAME.loc[test_train[0][0]].append(DEVICE_FRAME.loc[test_train[0][1]])\n",
    "    PRED = pd.DataFrame(index=test.index,columns=test.columns)\n",
    "    for j in range(0,len(test.columns)):\n",
    "        for i in range(0, len(pred)):\n",
    "            PRED.iloc[i][j] = pred[i][j]\n",
    "    SUM_VAR = PRED#-train.iloc[-1]\n",
    "    COLS = get_top_cols(SUM_TRUE, -1, 5)\n",
    "    COLOR_DICT = get_color_dict(COLS)\n",
    "    _, ax = plt.subplots()\n",
    "    train.append(SUM_TRUE).plot(ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "    SUM_VAR.plot(linestyle='dashed', ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.scatter(DEVICE_FRAME.index, DEVICE_FRAME[\"orders\"].cumsum())\n",
    "plt.figure(figsize=(20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "TRAIN_SIZE = -336#int(len(DEVICE_FRAME)*TRAIN_PERCENT)\n",
    "TRAIN_Y = DEVICE_FRAME[:TRAIN_SIZE]\n",
    "TRAIN_Y = TRAIN_Y.loc[:, (TRAIN_Y != TRAIN_Y.iloc[0]).any()]\n",
    "VAL_Y = DEVICE_FRAME[TRAIN_Y.columns][TRAIN_SIZE:]\n",
    "#TRAIN_X = DEVICE_FRAME[\"day\"]\n",
    "#VAL_X = DEVICE_FRAME[\"day\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creating the train and validation set\n",
    "train = TRAIN_Y\n",
    "valid = VAL_Y\n",
    "naive_pred = DEVICE_FRAME[TRAIN_SIZE-1:-1]\n",
    "\n",
    "#fit the model\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "model = VAR(endog=train)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction on validation\n",
    "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=valid.index,columns=VAL_Y.columns)\n",
    "for j in range(0,len(VAL_Y.columns)):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "#check rmse\n",
    "print(\"MAE VAR, MAE Naive, Min Val, Max Val\")\n",
    "MIN = valid.min()\n",
    "MAX = valid.max()\n",
    "SUM_MAE = 0\n",
    "for i in pred.columns:\n",
    "    SUM_MAE += mean_absolute_error(pred[i], valid[i])\n",
    "    print('MAE value for', str(i)+':\\t', mean_absolute_error(pred[i], valid[i]), \"\\t\", mean_absolute_error(naive_pred[i], valid[i]), \"\\t\", MIN[i], \"\\t\", MAX[i])\n",
    "print(\"AVG MAE:\", (SUM_MAE/len(VAL_Y.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=valid.index,columns=DEVICE_FRAME.columns)\n",
    "for j in range(0,len(DEVICE_FRAME.columns)):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "#check rmse\n",
    "print(\"RMSE for VAR predictions and Naive Predictions\")\n",
    "for i in DEVICE_FRAME.columns:\n",
    "    print('rmse value for', i, 'is : ', sqrt(mean_squared_error(pred[i], valid[i])), \"\\t\", sqrt(mean_squared_error(naive_pred[i], valid[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "DEVICE_FRAME = sc.fit_transform(DEVICE_FRAME)\n",
    "DEVICE_FRAME.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}