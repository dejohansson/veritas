{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm, markers\n",
    "import json\n",
    "from itertools import zip_longest, product\n",
    "import os\n",
    "from math import sqrt\n",
    "import holidays\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# instance_varibles\n",
    "# GLOBAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas display options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"veritas_data/parser_menus/menu.json\", \"r\") as f:\n",
    "    MENU = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_store_id_to_name(file_name):\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_stores = json.load(f)\n",
    "        id_to_name = {store[\"Id\"] : store[\"Name\"] for store in raw_stores}\n",
    "    return id_to_name\n",
    "\n",
    "STORE_ID_TO_NAME = get_store_id_to_name(\"veritas_data/stores/all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_ID_TO_NAME[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date, fiters out all dates before start_date\n",
    "# date_unit, determines the unit of the date column if date is a number\n",
    "def load_ts_data(file_name, start_date=None, date_unit=None):\n",
    "    print(\"Loading main columns...\")\n",
    "    df = pd.read_csv(file_name, usecols=[\"device_id\", \"date\", \"store_id\", \"class\", \"total_price\"], header=0)\n",
    "\n",
    "    #print(\"Formatting main columns...\")\n",
    "    #df[\"device_id\"], device_map = df[\"device_id\"].factorize()\n",
    "    #df[\"store_id\"], store_map = df[\"store_id\"].factorize()\n",
    "    #df[\"class\"], class_map = df[\"class\"].factorize()\n",
    "    #df = df.astype(\"uint32\")\n",
    "\n",
    "    print(\"Loading products...\")\n",
    "    with open(file_name, \"r\") as f:\n",
    "        num_columns = len(f.readline().split(\",\"))\n",
    "    for cols in zip_longest(*(iter(range(5, num_columns)),) * 50):\n",
    "        cols = [c for c in cols if c is not None]\n",
    "        temp_df = pd.read_csv(file_name, usecols=list(cols), header=0, dtype=\"uint8\")\n",
    "        temp_df = temp_df.loc[:, (temp_df != 0).any(axis=0)]   # Remove \"zero\" columns\n",
    "        df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "    wl = [col for col in df.columns if col not in ['device_id', 'store_id', 'class', \"date\"]]\n",
    "    df = df.drop(df[df[wl].eq(0).all(axis=1)].index)\n",
    "    if date_unit is None:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    else:\n",
    "        df['date'] = pd.to_datetime(df['date'], unit=date_unit)\n",
    "    if start_date is not None:\n",
    "            df.drop(df[df[\"date\"] < start_date].index, inplace=True)\n",
    "    df.set_index([\"store_id\", \"device_id\", \"date\"], inplace=True)\n",
    "    return df#, device_map, store_map, class_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TS_FILE = \"veritas_data/post_parser_orders/device_time_series_2020_01-01_to_02-19.csv\"\n",
    "TSF = load_ts_data(TS_FILE, start_date=\"2020-01-01\", date_unit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSF.drop(\"class\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TSF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COL_WL = [col for col in TSF.columns if col not in [\"device_id\", \"store_id\", \"class\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, path):\n",
    "    if not os.path.isfile(path):\n",
    "        df.to_csv(path)\n",
    "    else:\n",
    "        print(path, \"already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns the device_id, number of orders and the data frame of the device with the most orders\n",
    "def get_top_devices(device_id_list, ts_frame, n):\n",
    "    top_devices = []\n",
    "    for device_id in device_id_list:\n",
    "        df = ts_frame.loc[ts_frame[\"device_id\"] == device_id, [\"store_id\"]]\n",
    "        try:\n",
    "            store_id = df.values[0][0]\n",
    "        except:\n",
    "            continue\n",
    "        num_orders = len(df.index)\n",
    "        top_devices.append((device_id, num_orders, store_id))\n",
    "        top_devices = sorted(top_devices, key=lambda d: d[1], reverse=True)[:n]\n",
    "    return top_devices\n",
    "\n",
    "# Returns df without rows where col_name equals a value occuring less than threshold times\n",
    "def trim_low_occurance_values(df, col_name, threshold):\n",
    "    s = df[col_name].value_counts().ge(threshold)\n",
    "    return df[df[col_name].isin(s[s].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "def get_intervals(df, t_delta):\n",
    "    first_date = df.index.min()[2]\n",
    "    last_date = df.index.max()[2]\n",
    "    time_step = last_date-t_delta\n",
    "    intervals = []\n",
    "    while time_step > first_date:\n",
    "        start = time_step\n",
    "        end = time_step+t_delta\n",
    "        intervals.append((start, end))\n",
    "        time_step -= (t_delta*max(min(np.random.normal(0.5, 0.05), 1), 0.4))\n",
    "    intervals.append((first_date, first_date+t_delta))\n",
    "    return intervals\n",
    "\n",
    "def sparsity_coefficient(df):\n",
    "    return 1.0 - (np.count_nonzero(df) / np.product(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "def reindex_by_date(df, start, end, freq, fill_value=0):\n",
    "    dates = pd.date_range(start=start, end=end, freq=freq)\n",
    "    index = list(set(df.index.droplevel(\"date\").tolist()))\n",
    "    index.sort()\n",
    "    dates = [(i, j, d) for i, j in index for d in dates]\n",
    "    df = df.reindex(dates).fillna(0)\n",
    "    return df\n",
    "\n",
    "def flatten_ts(df):\n",
    "    return df.values.flatten()\n",
    "\n",
    "# \n",
    "def format_ts(df, start, end, freq, whitelist=[], flatten=False, normalize=False, sales_percentage=False):\n",
    "    product_cols = [col for col in df.columns if \"product\" in col]\n",
    "    df.rename(columns={\"total_price\": \"average_price\"}, inplace=True)\n",
    "    df[\"num_orders\"] = 1\n",
    "    agg_dict = {col_name: np.sum for col_name in df.columns}\n",
    "    agg_dict.update({\"average_price\": np.mean})\n",
    "    df = df.groupby([df.index.get_level_values(i) for i in [0,1]]+[pd.Grouper(freq=freq, level=-1)]).agg(agg_dict)\n",
    "    df = reindex_by_date(df, df.index.get_level_values(\"date\")[0], df.index.get_level_values(\"date\")[-1], freq)\n",
    "    if sales_percentage:\n",
    "        df[product_cols] = df[product_cols].div(df[product_cols].sum(axis=1), axis=0)\n",
    "    if normalize:\n",
    "        df -= df.min()\n",
    "        df /= df.max()\n",
    "    df = df.fillna(0)\n",
    "    if flatten:\n",
    "        df.index.droplevel(\"store_id\")\n",
    "        device_index = df.index.droplevel(\"date\").drop_duplicates()\n",
    "        df = df.groupby(level=\"device_id\").apply(flatten_ts)\n",
    "        df = pd.DataFrame(np.stack(df), index=device_index)\n",
    "    return df\n",
    "\n",
    "def correlation_matrix(df, start, end, freq, whitelist, dist_func, normalize=False, sales_percentage=False):\n",
    "    start_time = time.time()\n",
    "    ids = df.index.get_level_values(\"device_id\").unique()\n",
    "    err_df = pd.DataFrame(index=ids, columns=ids)\n",
    "\n",
    "    print(\"Formatting devices...\")\n",
    "    df = format_ts(df, start, end, freq, whitelist, normalize=normalize, sales_percentage=sales_percentage, flatten=True)\n",
    "\n",
    "    print(\"Generating correlation matrix...\")\n",
    "    err_df = pd.DataFrame(squareform(pdist(df, metric=\"euclidean\")), columns=df.index, index=df.index)\n",
    "    print(\"Done!\", \"\\tElapsed time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return err_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Score Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cophenetic Testing\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, cophenet\n",
    "\n",
    "def cophenet_test(df, intervals, freqs, metrics, methods, pcas, whitelist=[], verbose=False):\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    if verbose:\n",
    "        print(\"Running tests...\")\n",
    "    windows = [(l, w) for l in intervals for w in get_intervals(df, l)]\n",
    "    n_tests = len(windows)*len(freqs)*len(metrics)*len(methods)*len(pcas)\n",
    "    if n_tests == 0:\n",
    "        print(\"Parameters generated zero tests!!!\")\n",
    "        return\n",
    "    tests_completed = 0\n",
    "\n",
    "    for interval, (start, end) in windows:\n",
    "        window = df[(df.index.get_level_values(\"date\") > start) & (df.index.get_level_values(\"date\") <= end)]\n",
    "        n_devices = window.index.get_level_values(\"device_id\").nunique()\n",
    "        if n_devices > 1:\n",
    "            for freq in freqs:\n",
    "                formatted_window = format_ts(window, start, end, freq, whitelist, flatten=True, normalize=True)\n",
    "                for n_pca in pcas:\n",
    "                    try:\n",
    "                        window_components = TruncatedSVD(n_components=n_pca).fit_transform(formatted_window)\n",
    "                        for metric in metrics:\n",
    "                            dist = pdist(window_components, metric=metric)\n",
    "                            for method in methods:\n",
    "                                if method != \"ward\" or metric == \"euclidean\":\n",
    "                                    link = linkage(dist, method=method)\n",
    "                                    score = cophenet(link, dist)[0]\n",
    "                                    scores.append((interval, freq, metric, method, n_pca, score))\n",
    "                                tests_completed += 1\n",
    "                                if verbose:\n",
    "                                    time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                                    print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "                    except ValueError:\n",
    "                        break\n",
    "        else:\n",
    "            tests_completed += n_tests//len(windows)\n",
    "            if verbose:\n",
    "                time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "                            \n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\nDone! Total time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "def k_means(x, n_clust):\n",
    "    return KMeans(n_clusters=n_clust, random_state=0, n_jobs=-1, verbose=2).fit(x).labels_\n",
    "\n",
    "def optics(x, min_samples):\n",
    "    return OPTICS(min_samples=min_samples, metric=\"euclidean\").fit(x).labels_\n",
    "\n",
    "def agglomerative_complete(x, n_clust):\n",
    "    dist = pdist(x, metric=\"euclidean\")\n",
    "    link = linkage(dist, method=\"complete\")\n",
    "    return fcluster(link, t=n_clust, criterion=\"maxclust\").astype(\"float64\")\n",
    "\n",
    "def agglomerative_single(x, n_clust):\n",
    "    dist = pdist(x, metric=\"euclidean\")\n",
    "    link = linkage(dist, method=\"single\")\n",
    "    return fcluster(link, t=n_clust, criterion=\"maxclust\").astype(\"float64\")\n",
    "\n",
    "def agglomerative_ward(x, n_clust):\n",
    "    dist = pdist(x, metric=\"euclidean\")\n",
    "    link = linkage(dist, method=\"ward\")\n",
    "    return fcluster(link, t=n_clust, criterion=\"maxclust\").astype(\"float64\")\n",
    "    \n",
    "def store_clust(x):\n",
    "    return x.index.get_level_values(\"store_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store based clustering\n",
    "# 100 PCA!\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def clust_test(df, intervals, freqs, clust_funcs, verbose=False):\n",
    "    start_time = time.time()\n",
    "    scores = []\n",
    "    if verbose:\n",
    "        print(\"Running tests...\")\n",
    "    windows = [(l, w) for l in intervals for w in get_intervals(df, l)]\n",
    "    n_tests = len(windows)*len(freqs)*len(clust_funcs)\n",
    "    if n_tests == 0:\n",
    "        print(\"Parameters generated zero tests!!!\")\n",
    "        return\n",
    "    tests_completed = 0\n",
    "\n",
    "    for interval, (start, end) in windows:\n",
    "        window = df[(df.index.get_level_values(\"date\") > start) & (df.index.get_level_values(\"date\") <= end)]\n",
    "        n_devices = window.index.get_level_values(\"device_id\").nunique()\n",
    "        if n_devices > 1:\n",
    "            for freq in freqs:\n",
    "                formatted_window = format_ts(window, start, end, freq, [], flatten=True, normalize=True)    \n",
    "                svd = TruncatedSVD(n_components=min(1000, formatted_window.shape[1]-1), random_state=0)\n",
    "                svd.fit(formatted_window)\n",
    "                variance = svd.explained_variance_ratio_.cumsum()\n",
    "                window_components = svd.transform(formatted_window)[:,variance < 0.96]\n",
    "                n_components = window_components.shape[1]\n",
    "                dist = pdist(window_components, metric=\"euclidean\")\n",
    "                for clust_func in clust_funcs:\n",
    "                    si_score = ch_score = np.NINF \n",
    "                    db_score = np.inf\n",
    "                    si_n_clust = ch_n_clust = db_n_clust = None\n",
    "                    si_n_outliers = ch_n_outliers = db_n_outliers = None\n",
    "                    if clust_func == optics:\n",
    "                        for min_samples in range(2, min(100, n_devices//20), 2):\n",
    "                            clust_labels = clust_func(window_components, min_samples)\n",
    "                            mask = clust_labels != -1\n",
    "                            n_outliers = np.count_nonzero(clust_labels == -1)\n",
    "                            clust_labels = clust_labels[mask]\n",
    "                            n_clust = len(np.unique(clust_labels))\n",
    "                            if n_clust < 2 : continue\n",
    "                            si = silhouette_score(squareform(dist)[mask][:,mask], clust_labels)\n",
    "                            if si > si_score:\n",
    "                                si_score = si\n",
    "                                si_n_clust = n_clust\n",
    "                                si_n_outliers = n_outliers\n",
    "                            ch = calinski_harabasz_score(squareform(dist)[mask][:,mask], clust_labels)\n",
    "                            if ch > ch_score:\n",
    "                                ch_score = ch\n",
    "                                ch_n_clust = n_clust\n",
    "                                ch_n_outliers = n_outliers\n",
    "                            db = davies_bouldin_score(squareform(dist)[mask][:,mask], clust_labels)\n",
    "                            if db < db_score:\n",
    "                                db_score = db\n",
    "                                db_n_clust = n_clust\n",
    "                                db_n_outliers = n_outliers\n",
    "                    elif clust_func == store_clust:\n",
    "                        clust_labels = clust_func(formatted_window)\n",
    "                        si_score = silhouette_score(squareform(dist), clust_labels)\n",
    "                        ch_score = calinski_harabasz_score(squareform(dist), clust_labels)\n",
    "                        db_score = davies_bouldin_score(squareform(dist), clust_labels)\n",
    "                        si_n_clust = ch_n_clust = db_n_clust = len(np.unique(clust_labels))\n",
    "                    else:\n",
    "                        for n_clust in range(2, min(100, n_devices//2)):\n",
    "                            clust_labels = clust_func(window_components, n_clust)\n",
    "                            si = silhouette_score(squareform(dist), clust_labels)\n",
    "                            if si > si_score:\n",
    "                                si_score = si\n",
    "                                si_n_clust = n_clust\n",
    "                            ch = calinski_harabasz_score(squareform(dist), clust_labels)\n",
    "                            if ch > ch_score:\n",
    "                                ch_score = ch\n",
    "                                ch_n_clust = n_clust\n",
    "                            db = davies_bouldin_score(squareform(dist), clust_labels)\n",
    "                            if db < db_score:\n",
    "                                db_score = db\n",
    "                                db_n_clust = n_clust\n",
    "                    scores.append((interval, freq, clust_func.__name__, si_score, ch_score, db_score, si_n_clust, ch_n_clust, db_n_clust, n_components, si_n_outliers, ch_n_outliers, db_n_outliers))\n",
    "                    tests_completed += 1\n",
    "                    if verbose:\n",
    "                        time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                        print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")\n",
    "        else:\n",
    "            tests_completed += n_tests//len(windows)\n",
    "            if verbose:\n",
    "                time_est = (n_tests-tests_completed) * (datetime.timedelta(seconds=(time.time()-start_time)) / tests_completed)\n",
    "                print(\"\\rTest:\", str(tests_completed) + \"/\" + str(n_tests), \"\\tTime remaining (est):\", str(time_est), end=\"\")        \n",
    "    if verbose:\n",
    "        print(\"\\nDone! Total time:\", str(datetime.timedelta(seconds=(time.time()-start_time))))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start, end = get_intervals(TSF, pd.Timedelta(1, unit=\"D\"))[0]\n",
    "\n",
    "window = TSF[(TSF.index.get_level_values(\"date\") > start) & (TSF.index.get_level_values(\"date\") <= end)]\n",
    "formatted_window = format_ts(window, start, end, \"1H\", [], flatten=True, normalize=True)\n",
    "formatted_window.shape    \n",
    "svd = TruncatedSVD(n_components=min(1000, formatted_window.shape[1]-1), random_state=0)\n",
    "svd.fit(formatted_window)\n",
    "variance = svd.explained_variance_ratio_.cumsum()\n",
    "print(len(variance), len(svd.transform(formatted_window)))\n",
    "window_components = svd.transform(formatted_window)[:,(variance < 0.96)]\n",
    "n_components = len(window_components)\n",
    "print(svd.transform(formatted_window).shape[1], window_components.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 3, 7, 14, 48 Days\n",
    "# 1, 12, 24, Hours\n",
    "\n",
    "TEST_NAME = \"6-methods_interval-1D-28D_freq-1H-24H_SVD-95-percent_formatted\"\n",
    "TEST_RES_PATH = \"./test_results/\" + TEST_NAME + \".csv\"\n",
    "#TEST_INDEX = [\"interval\", \"frequency\", \"metric\", \"method\", \"n_pca\"]\n",
    "TEST_INDEX = [\"interval\", \"frequency\", \"clust_func\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FREQ_UNIT = \"H\"\n",
    "INTERVALS = [pd.Timedelta(n, unit=\"D\") for n in [1, 3, 7, 14, 28]]\n",
    "FREQS = [str(n)+FREQ_UNIT for n in [1, 12, 24]]\n",
    "DIST_METRICS = [\"euclidean\"]\n",
    "LINK_METHODS = [\"average\"]\n",
    "PCAS = [20, 60, 100, 200, 300]\n",
    "METHODS = [k_means, optics, agglomerative_complete, agglomerative_ward, agglomerative_single, store_clust]\n",
    "\n",
    "TEST_RES = clust_test(TSF, INTERVALS, FREQS, METHODS, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF = pd.DataFrame(TEST_RES, columns=TEST_INDEX+[\"si_score\", \"ch_score\", \"db_score\", \"si_n_clust\", \"ch_n_clust\", \"db_n_clust\", \"n_components\", \"si_n_outliers\", \"ch_n_outliers\", \"db_n_outliers\"])\n",
    "TEST_DF.set_index(TEST_INDEX, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(TEST_DF, TEST_RES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF = pd.read_csv(TEST_RES_PATH)\n",
    "#TEST_DF = pd.read_csv(\"./test_results/\" + \"frequency_test_2W_30min_to_10080min\" + \".csv\")\n",
    "\n",
    "#TEST_DF = pd.concat([pd.read_csv(\"./test_results/\" + \"frequency_test_1W_30min_to_4320min\" + \".csv\"), pd.read_csv(\"./test_results/\" + \"frequency_test_1W_4350min_to_10080min\" + \".csv\")])\n",
    "TEST_DF.set_index(TEST_INDEX, inplace=True)\n",
    "TEST_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF.replace({np.inf: None, np.NINF: None}, inplace=True)\n",
    "TEST_DF.drop([\"n_components\",\"si_n_outliers\", \"ch_n_outliers\", \"db_n_outliers\"], axis=1, inplace=True)\n",
    "F_TEST_DF = pd.DataFrame(index=TEST_DF.index.drop_duplicates())\n",
    "F_TEST_DF[['si_score_mean', 'ch_score_mean', 'db_score_mean', 'si_n_clust_mean', 'ch_n_clust_mean', 'db_n_clust_mean']] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))['si_score', 'ch_score', 'db_score', 'si_n_clust', 'ch_n_clust', 'db_n_clust'].mean()\n",
    "F_TEST_DF[['si_score_median', 'ch_score_median', 'db_score_median', 'si_n_clust_median', 'ch_n_clust_median', 'db_n_clust_median']] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))['si_score', 'ch_score', 'db_score', 'si_n_clust', 'ch_n_clust', 'db_n_clust'].median()\n",
    "F_TEST_DF[['si_score_min', 'ch_score_min', 'db_score_min', 'si_n_clust_min', 'ch_n_clust_min', 'db_n_clust_min']] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))['si_score', 'ch_score', 'db_score', 'si_n_clust', 'ch_n_clust', 'db_n_clust'].min()\n",
    "F_TEST_DF[['si_score_max', 'ch_score_max', 'db_score_max', 'si_n_clust_max', 'ch_n_clust_max', 'db_n_clust_max']] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))['si_score', 'ch_score', 'db_score', 'si_n_clust', 'ch_n_clust', 'db_n_clust'].max()\n",
    "#F_TETS_DF = F_TETS_DF.groupby(level=list(range(len(TEST_INDEX)))).mean()\n",
    "F_TEST_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(F_TEST_DF, TEST_RES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DF[\"mean_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].mean()\n",
    "TEST_DF[\"median_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].median()\n",
    "TEST_DF[\"max_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].max()\n",
    "TEST_DF[\"min_cophenetic_score\"] = TEST_DF.groupby(level=list(range(len(TEST_INDEX))))[\"cophenet_score\"].min()\n",
    "TEST_DF.drop([\"cophenet_score\"], axis=1, inplace=True)\n",
    "TEST_DF = TEST_DF.loc[~TEST_DF.index.duplicated(keep=\"first\")]\n",
    "TEST_DF.index = TEST_DF.index.set_levels(TEST_DF.index.levels[TEST_DF.index.names.index(\"frequency\")].str.replace(\"H\", \"\").astype(\"int32\"), level=\"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot column\n",
    "COLUMN_LABEL = \"n_pca\"\n",
    "TEST_DF = TEST_DF.sort_values(by=[COLUMN_LABEL], ascending=True)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.xticks(TEST_DF.index.get_level_values(COLUMN_LABEL).to_series())\n",
    "plt.scatter(TEST_DF.index.get_level_values(COLUMN_LABEL), TEST_DF[\"mean_cophenetic_score\"])\n",
    "plt.axhline(y=TEST_DF[\"mean_cophenetic_score\"].max(), color='r', linestyle='-')\n",
    "plt.axvline(x=TEST_DF[\"mean_cophenetic_score\"].idxmax()[TEST_DF.index.names.index(COLUMN_LABEL)], color='r', linestyle='-')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,25))\n",
    "ax = sns.heatmap(TEST_DF.sort_values(by=[\"mean_cophenetic_score\"], ascending=False), cmap=sns.cm.rocket_r, annot=True, fmt=\".4g\") #.sort_values(by=[\"mean_cophenetic_score\"], ascending=False)\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.label.set_size(16)\n",
    "ax.set_title(\"Hierarchical clustering comparison\", fontsize=24)\n",
    "ax.text(0.5, -0.05, \"Cophonetic Correlation Score (higher is better)\\nSample frequency: 1 hour,    Interval length: 1 week,    Number of intervals: 8\",\n",
    "        verticalalignment=\"bottom\", horizontalalignment=\"center\",\n",
    "        transform=ax.transAxes, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_METRICS = [\"euclidean\", \"cityblock\", \"cosine\", \"correlation\"]\n",
    "LINK_METHODS = [\"ward\", \"complete\", \"average\", \"single\", \"weighted\", \"centroid\", \"median\"]\n",
    "INTERVAL = pd.Timedelta(1, unit=\"W\")\n",
    "FREQ = \"1H\"\n",
    "COPHENET_SCORES = cophenet_test(TSF, INTERVAL, FREQ, DIST_METRICS, LINK_METHODS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP_DF = pd.DataFrame(COPHENET_SCORES, columns=[\"metric\", \"method\", \"cophenet_score\"])\n",
    "COP_DF.set_index([\"metric\", \"method\"], inplace=True)\n",
    "COP_DF[\"mean_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].mean()\n",
    "COP_DF[\"median_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].median()\n",
    "COP_DF[\"max_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].max()\n",
    "COP_DF[\"min_cophenetic_score\"] = COP_DF.groupby([\"metric\", \"method\"])[\"cophenet_score\"].min()\n",
    "COP_DF.drop([\"cophenet_score\"], axis=1, inplace=True)\n",
    "COP_DF = COP_DF.loc[~COP_DF.index.duplicated(keep='first')]\n",
    "\n",
    "plt.figure(figsize=(12,15), )\n",
    "ax = sns.heatmap(COP_DF.sort_values(by=[\"mean_cophenetic_score\"], ascending=False), cmap=sns.cm.rocket_r, annot=True, fmt=\".4g\")\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.label.set_size(16)\n",
    "ax.set_title(\"Hierarchical clustering comparison\", fontsize=24)\n",
    "ax.text(0.5, -0.05, \"Cophonetic Correlation Score (higher is better)\\nSample frequency: 1 hour,    Interval length: 1 week,    Number of intervals: 8\",\n",
    "        verticalalignment=\"bottom\", horizontalalignment=\"center\",\n",
    "        transform=ax.transAxes, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA/SVD Analysis\n",
    "Investigate optimal number of PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PCA/SVD Analysis\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, cophenet\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mayavi.mlab as myalab\n",
    "\n",
    "SAMPLE_FREQUENCY = \"1H0min\"\n",
    "START, END = get_intervals(TSF, pd.Timedelta(1, unit=\"D\"))[15]\n",
    "print(START, \"--\", END)\n",
    "INTERVAL = TSF[(TSF.index.get_level_values(\"date\") > START) & (TSF.index.get_level_values(\"date\") <= END)]\n",
    "DF = format_ts(INTERVAL, START, END, SAMPLE_FREQUENCY, flatten=True, normalize=False)\n",
    "\n",
    "SVD_COMPONENTS = TruncatedSVD(n_components=DF.shape[1]-1).fit(DF).explained_variance_ratio_.cumsum()\n",
    "PCA_COMPONENTS = PCA().fit(DF).explained_variance_ratio_.cumsum()\n",
    "NORM_SVD_COMPONENTS = TruncatedSVD(n_components=DF.shape[1]-1).fit(scale(DF)).explained_variance_ratio_.cumsum()\n",
    "NORM_PCA_COMPONENTS = PCA().fit(scale(DF)).explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(SVD_COMPONENTS, label=\"SVD\", linestyle=\"--\")\n",
    "#plt.show()\n",
    "plt.plot(PCA_COMPONENTS, label=\"PCA\", linestyle=\":\")\n",
    "#plt.show()\n",
    "plt.plot(NORM_SVD_COMPONENTS, label=\"SVD_NORM\", linestyle=\"--\")\n",
    "#plt.show()\n",
    "plt.plot(NORM_PCA_COMPONENTS, label=\"PCA_NORM\", linestyle=\":\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE/SVD Visualization\n",
    "A visual comparison between store segmentation and auto segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "def silhouette_clusters(distance_matrix, labels, metric=\"precomputed\"):\n",
    "    sample_sil_score = silhouette_samples(distance_matrix, labels, metric=metric)\n",
    "    label_sil_score = pd.DataFrame(np.stack((labels, sample_sil_score), axis=1), columns=[\"Label\", \"Score\"])\n",
    "    label_sil_score = label_sil_score.groupby(\"Label\").mean()\n",
    "    return label_sil_score\n",
    "\n",
    "# STORE_SIL_SCORE = silhouette_clusters(squareform(DIST), STORE_LABELS)\n",
    "# STORE_SIL_SCORE.rename(index=STORE_ID_TO_NAME, inplace=True)\n",
    "# STORE_SIL_SCORE.sort_values(\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram, cophenet\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import mayavi.mlab as myalab\n",
    "\n",
    "SAMPLE_FREQUENCY = \"3H0min\"\n",
    "START, END = get_intervals(TSF, pd.Timedelta(7, unit=\"D\"))[0]\n",
    "print(START, \"--\", END)\n",
    "INTERVAL = TSF[(TSF.index.get_level_values(\"date\") > START) & (TSF.index.get_level_values(\"date\") <= END)]\n",
    "print(\"Raw data sparsity\", sparsity_coefficient(INTERVAL))\n",
    "N_STORES = INTERVAL.index.get_level_values(\"store_id\").nunique()\n",
    "print(\"N_STORES:\", N_STORES)\n",
    "N_DEVICES = INTERVAL.index.get_level_values(\"device_id\").nunique()\n",
    "DF = format_ts(INTERVAL, START, END, SAMPLE_FREQUENCY, flatten=True, normalize=True)\n",
    "print(\"Formatted data sparsity\", sparsity_coefficient(DF))\n",
    "STORE_COLOR_MAP_2 = DF.index.get_level_values(\"store_id\")\n",
    "\n",
    "DF = scale(DF)\n",
    "print(\"Normalized data sparsity\", sparsity_coefficient(DF))\n",
    "\n",
    "SVD_COMPONENTS = TruncatedSVD(n_components=50).fit_transform(DF)\n",
    "print(\"SVD sparsity\", sparsity_coefficient(SVD_COMPONENTS))\n",
    "#SVD_COMPONENTS -= SVD_COMPONENTS.min()\n",
    "#SVD_COMPONENTS /= SVD_COMPONENTS.max()\n",
    "\n",
    "PCA_COMPONENTS_10D = PCA(n_components=10).fit_transform(SVD_COMPONENTS)\n",
    "PCA_COMPONENTS_3D = PCA(n_components=2).fit_transform(SVD_COMPONENTS)\n",
    "PCA_COMPONENTS_3D -= PCA_COMPONENTS_3D.min()\n",
    "PCA_COMPONENTS_3D /= PCA_COMPONENTS_3D.max()\n",
    "TSNE_3D = TSNE(n_components=2, perplexity=10, learning_rate=200, n_iter=10000, n_iter_without_progress=500, verbose=2, n_jobs=-1).fit_transform(PCA_COMPONENTS_10D)\n",
    "TSNE_3D -= TSNE_3D.min()\n",
    "TSNE_3D /= TSNE_3D.max()\n",
    "\n",
    "NUMBER_OF_COLORS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optics(TSNE_3D, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIST = pdist(TSNE_3D, metric=\"euclidean\")\n",
    "LINK = linkage(DIST, method=\"ward\")\n",
    "CLUST = fcluster(LINK, t=125, criterion=\"maxclust\").astype(\"float64\")\n",
    "CLUST = optics(TSNE_3D, 5)\n",
    "# NUMBER_OF_COLORS = 21\n",
    "# START_CLUST = 0\n",
    "# CLUST_COLOR_MAP = np.copy(CLUST)\n",
    "# CLUST_COLOR_MAP[(CLUST_COLOR_MAP < START_CLUST) | (CLUST_COLOR_MAP > START_CLUST+NUMBER_OF_COLORS-1)] = 0.0\n",
    "# CLUST_COLOR_MAP[CLUST_COLOR_MAP > 0] -= (START_CLUST-1)\n",
    "# CLUST_COLOR_MAP -= CLUST_COLOR_MAP.min()\n",
    "# CLUST_COLOR_MAP /= CLUST_COLOR_MAP.max()\n",
    "\n",
    "STORE_SIL_SCORE = silhouette_clusters(squareform(DIST), CLUST).mean()\n",
    "print(STORE_SIL_SCORE)\n",
    "\n",
    "CLUST_COLOR_MAP = pd.DataFrame(np.copy(CLUST))\n",
    "CLUST_COLOR_MAP[~(CLUST_COLOR_MAP.isin(range(1,20)))] = 0.0\n",
    "CLUST_COLOR_MAP = CLUST_COLOR_MAP.values.T[0]\n",
    "CLUST_COLOR_MAP -= CLUST_COLOR_MAP.min()\n",
    "CLUST_COLOR_MAP /= CLUST_COLOR_MAP.max()\n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.scatter(TSNE_3D.T[0].T, TSNE_3D.T[1].T, c=CLUST_COLOR_MAP, cmap=\"tab20\")\n",
    "plt.colorbar(orientation=\"vertical\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Clusters \"+\"n=\"+str(125)+\" (showing 19)\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORE_LABELS = INTERVAL.index[~INTERVAL.index.get_level_values(\"device_id\").duplicated()].get_level_values(\"store_id\").astype(\"float64\").values\n",
    "# START_COLOR = 50\n",
    "# STORE_COLOR_MAP = np.copy(STORE_LABELS)\n",
    "# STORE_COLOR_MAP[(STORE_COLOR_MAP < START_COLOR) | (STORE_COLOR_MAP > START_COLOR+NUMBER_OF_COLORS-1)] = 0.0\n",
    "# STORE_COLOR_MAP[STORE_COLOR_MAP > 0] -= (START_COLOR-1)\n",
    "# STORE_COLOR_MAP -= STORE_COLOR_MAP.min()\n",
    "# STORE_COLOR_MAP /= STORE_COLOR_MAP.max()\n",
    "\n",
    "STORE_SIL_SCORE = silhouette_clusters(squareform(DIST), STORE_LABELS)\n",
    "STORE_SIL_SCORE.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "TOP_DICT = {old:(new+1) for new, old in enumerate(STORE_SIL_SCORE.iloc[np.r_[0:4, -4:-0]].index.values)}\n",
    "STORE_COLOR_MAP = pd.DataFrame(np.copy(STORE_LABELS))\n",
    "\n",
    "STORE_COLOR_MAP[~(STORE_COLOR_MAP.isin(TOP_DICT.keys()))] = 0.0\n",
    "STORE_COLOR_MAP.replace({0: TOP_DICT}, inplace=True)\n",
    "STORE_COLOR_MAP = STORE_COLOR_MAP.values.T[0]\n",
    "STORE_COLOR_MAP -= STORE_COLOR_MAP.min()\n",
    "STORE_COLOR_MAP /= STORE_COLOR_MAP.max()\n",
    "\n",
    "STORE_SIL_SCORE.rename(index=STORE_ID_TO_NAME, inplace=True)\n",
    "print(STORE_SIL_SCORE.mean())\n",
    "\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.scatter(TSNE_3D.T[0].T, TSNE_3D.T[1].T, c=STORE_COLOR_MAP, cmap=\"Set1_r\")\n",
    "plt.colorbar(orientation=\"vertical\")\n",
    "plt.axis('off')\n",
    "plt.title(\"Stores \"+\"n=\"+str(N_STORES)+\" (showing 8)\", size=20)\n",
    "plt.show()\n",
    "\n",
    "STORE_SIL_SCORE.index.set_names(['Store'], inplace=True)\n",
    "\n",
    "ax = sns.heatmap(STORE_SIL_SCORE.iloc[np.r_[0:4, -4:-0]], cmap=sns.cm.rocket_r, annot=True, fmt=\".4g\")\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.yaxis.label.set_size(16)\n",
    "ax.set_title(\"Store Silhouette Score\", fontsize=24)\n",
    "#ax.text(0.5, -0.05, \"Cophonetic Correlation Score (higher is better)\\nSample frequency: 1 hour,    Interval length: 1 week,    Number of intervals: 8\", verticalalignment=\"bottom\", horizontalalignment=\"center\", transform=ax.transAxes, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_DIST = pdist(TSNE_3D, metric=\"euclidean\")\n",
    "STORE_SIL_SCORE = silhouette_clusters(squareform(STORE_DIST), STORE_LABELS)\n",
    "#STORE_SIL_SCORE.rename(index=STORE_ID_TO_NAME, inplace=True)\n",
    "STORE_SIL_SCORE.sort_values(\"Score\", ascending=False).iloc[np.r_[:5,-5:0]].index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST = pdist(SVD_COMPONENTS, metric=\"euclidean\")\n",
    "LINK = linkage(DIST, method=\"complete\")\n",
    "CLUST = fcluster(LINK, t=N_STORES, criterion=\"maxclust\").astype(\"float64\")\n",
    "START_CLUST = 0\n",
    "CLUST_COLOR_MAP = np.copy(CLUST)\n",
    "CLUST_COLOR_MAP[(CLUST_COLOR_MAP < START_CLUST) | (CLUST_COLOR_MAP > START_CLUST+NUMBER_OF_COLORS-1)] = 0.0\n",
    "CLUST_COLOR_MAP[CLUST_COLOR_MAP > 0] -= (START_CLUST-1)\n",
    "CLUST_COLOR_MAP -= CLUST_COLOR_MAP.min()\n",
    "CLUST_COLOR_MAP /= CLUST_COLOR_MAP.max()\n",
    "\n",
    "#nodes = myalab.points3d(TSNE_3D.T[0].T, TSNE_3D.T[1].T, TSNE_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes = myalab.points3d(PCA_COMPONENTS_3D.T[0].T, PCA_COMPONENTS_3D.T[1].T, PCA_COMPONENTS_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes.glyph.scale_mode = 'scale_by_vector'\n",
    "nodes.mlab_source.dataset.point_data.scalars = list(CLUST_COLOR_MAP)\n",
    "myalab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STORE_LABELS = INTERVAL.index[~INTERVAL.index.get_level_values(\"device_id\").duplicated()].get_level_values(\"store_id\").astype(\"float64\").values\n",
    "START_COLOR = 50\n",
    "STORE_COLOR_MAP = np.copy(STORE_LABELS)\n",
    "STORE_COLOR_MAP[(STORE_COLOR_MAP < START_COLOR) | (STORE_COLOR_MAP > START_COLOR+NUMBER_OF_COLORS-1)] = 0.0\n",
    "STORE_COLOR_MAP[STORE_COLOR_MAP > 0] -= (START_COLOR-1)\n",
    "STORE_COLOR_MAP -= STORE_COLOR_MAP.min()\n",
    "STORE_COLOR_MAP /= STORE_COLOR_MAP.max()\n",
    "\n",
    "#nodes = myalab.points3d(TSNE_3D.T[0].T, TSNE_3D.T[1].T, TSNE_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes = myalab.points3d(PCA_COMPONENTS_3D.T[0].T, PCA_COMPONENTS_3D.T[1].T, PCA_COMPONENTS_3D.T[2].T, scale_factor=0.01, colormap=\"Vega10\")\n",
    "nodes.glyph.scale_mode = 'scale_by_vector'\n",
    "nodes.mlab_source.dataset.point_data.scalars = list(STORE_COLOR_MAP)\n",
    "myalab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Silhoutte score explained\n",
    "def gauss_2d(center, sigma, n_points):\n",
    "    return np.random.multivariate_normal(center, ((sigma,0), (0, sigma)), n_points).T\n",
    "\n",
    "def plot_gauss_clusters(clusters, axis_limits, title=\"\"):\n",
    "    markers = [\"o\", \"v\", \"s\", \"^\"]\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.title(title)\n",
    "    plt.xlim(*axis_limits)\n",
    "    plt.ylim(*axis_limits)\n",
    "    for i, (x, y) in enumerate(clusters):\n",
    "        plt.scatter(x, y, label=\"Cluster \"+str(i+1), marker=markers[i%4])\n",
    "    plt.legend()\n",
    "    plt.axis('off')        \n",
    "    plt.show()\n",
    "\n",
    "def synth_silhouette(clusters):\n",
    "    arr = np.array([(i, x, y) for i, cluster in enumerate(clusters) for x, y in cluster.T])\n",
    "    df =  pd.DataFrame(arr, columns=[\"label\", \"x\", \"y\"])\n",
    "    dist = pdist(df[[\"x\", \"y\"]], metric=\"euclidean\")\n",
    "    return silhouette_score(squareform(dist), df[\"label\"])\n",
    "\n",
    "CLUSTERS = [gauss_2d((5.5,5.5), 0.02, 80), gauss_2d((5,4.5), 0.02, 80), gauss_2d((4.5,5.5), 0.02, 80)]\n",
    "plot_gauss_clusters(CLUSTERS, (4,6), \"Possitive Silhoutte Score\")\n",
    "print(synth_silhouette(CLUSTERS))\n",
    "\n",
    "CLUSTERS = [gauss_2d((5.1,5.1), 0.3, 80), gauss_2d((5,4.6), 0.3, 80), gauss_2d((4.6,5.1), 0.3, 80)]\n",
    "plot_gauss_clusters(CLUSTERS, (3,7), \"Zero Silhoutte Score\")\n",
    "print(synth_silhouette(CLUSTERS))\n",
    "\n",
    "CLUST_1 = np.concatenate([gauss_2d((5.5,5.5), 0.02, 20), gauss_2d((5,4.5), 0.02, 20), gauss_2d((4.5,5.5), 0.02, 20)], axis=1)\n",
    "CLUST_2 = np.concatenate([gauss_2d((5.5,5.5), 0.02, 20), gauss_2d((5,4.5), 0.02, 20), gauss_2d((4.5,5.5), 0.02, 20)], axis=1)\n",
    "CLUST_3 = np.concatenate([gauss_2d((5.5,5.5), 0.02, 20), gauss_2d((5,4.5), 0.02, 20), gauss_2d((4.5,5.5), 0.02, 20)], axis=1)\n",
    "CLUSTERS = [CLUST_1, CLUST_2, CLUST_3]\n",
    "plot_gauss_clusters(CLUSTERS, (4,6), \"Negative Silhoutte Score\")\n",
    "print(synth_silhouette(CLUSTERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "#Example plot of kiosk MVTS\n",
    "SAMPLE_FREQUENCY = \"1H0min\"\n",
    "START, END = get_intervals(TSF, pd.Timedelta(2, unit=\"D\"))[2]\n",
    "print(START, \"--\", END)\n",
    "INTERVAL = TSF[(TSF.index.get_level_values(\"date\") > START) & (TSF.index.get_level_values(\"date\") <= END)]\n",
    "DF = format_ts(INTERVAL, START, END, SAMPLE_FREQUENCY, flatten=False, normalize=True)\n",
    "DF = DF.xs(\"AwfulPerseveringIncome\", level=\"device_id\")\n",
    "TIME_VAL = list(DF.index.get_level_values(\"date\"))\n",
    "plt.figure(figsize=(14,6))\n",
    "ax = plt.gca()\n",
    "yearFmt = mdates.DateFormatter(\"%H:%M\")\n",
    "\n",
    "DF.unstack(level=0).plot(y=[\"product_256\"], ax=ax, label=[\"Kids meal\"], linestyle=\"--\")\n",
    "DF.unstack(level=0).plot(y=[\"product_144\"], ax=ax, label=[\"Chili cheese\"], linestyle=\"-.\")\n",
    "DF.unstack(level=0).plot(y=[\"product_394\"], ax=ax, label=[\"Smokey Chipotle Chicken Menu\"], linestyle=\":\")\n",
    "DF.unstack(level=0).plot(y=[\"average_price\"], ax=ax, label=[\"Average Purchase Value\"])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Values (0,1)\")\n",
    "ax.xaxis.set_major_formatter(yearFmt)\n",
    "xticks = ax.xaxis.get_major_ticks()\n",
    "xticks[0].label1.set_visible(False)\n",
    "xticks[-1].label1.set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "ax = plt.gca()\n",
    "DF.unstack(level=0).plot(ax=ax)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Values (0,1)\")\n",
    "ax.xaxis.set_major_formatter(yearFmt)\n",
    "xticks = ax.xaxis.get_major_ticks()\n",
    "xticks[0].label1.set_visible(False)\n",
    "xticks[-1].label1.set_visible(False)\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_cols(df, row_index, n_cols):\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    row_frame = df.iloc[row_index].copy()\n",
    "    columns = []\n",
    "    for _ in range(n_cols):\n",
    "        if len(row_frame) == 0:\n",
    "            break\n",
    "        col_id = row_frame.idxmax()\n",
    "        columns.append(col_id)\n",
    "        row_frame = row_frame.drop([col_id])\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_dict(labels):\n",
    "    #labels = [col for col in df.columns if col not in [\"device_id\", \"date\", \"store_id\", \"class\", \"total_price\"]]\n",
    "    colors = [cm.rainbow(i) for i in np.linspace(0, 1, len(labels))]\n",
    "    c_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        c_dict[label] = colors[i]\n",
    "    return c_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START_INDEX = 0\n",
    "N_COLS = 6\n",
    "COL_SET = set()\n",
    "for D in DEVICES:\n",
    "    DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == D[0], COL_WL].copy()\n",
    "    DEVICE_FRAME = DEVICE_FRAME.drop([\"total_price\"], axis=1)\n",
    "    DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").sum().iloc[START_INDEX:]\n",
    "    DEVICE_FRAME = DEVICE_FRAME.cumsum()\n",
    "    for C in get_top_cols(DEVICE_FRAME, -1, N_COLS):\n",
    "        COL_SET.add(C)\n",
    "COLOR_DICT = get_color_dict(COL_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for D in DEVICES:\n",
    "    DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == D[0], column_whitelist].copy()\n",
    "    if len(DEVICE_FRAME) == 0:\n",
    "        continue\n",
    "    DEVICE_FRAME = DEVICE_FRAME.drop([\"total_price\"], axis=1)\n",
    "    DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").sum().iloc[START_INDEX:]\n",
    "    DEVICE_FRAME = DEVICE_FRAME.cumsum()\n",
    "    COLS = get_top_cols(DEVICE_FRAME, -1, N_COLS)\n",
    "    fig = plt.figure()\n",
    "    DEVICE_FRAME.plot(kind='line',y=COLS, figsize=(16, 10), color=[COLOR_DICT.get(x, '#666666') for x in COLS], linewidth=3.0)\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.figtext(.5,.9, \"Device \"+str(D[0])+\", Store \"+str(D[2]),fontsize=24)\n",
    "    #plt.show()\n",
    "    plt.savefig(\"C:/Users/user/Desktop/store_/\"+str(D[2]+\"/\"+\"device_\"+str(D[0])+\"_day.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_cols(df):\n",
    "    df[\"hour\"] = df.index.hour\n",
    "    df[\"day_of_week\"] = df.index.dayofweek\n",
    "    df[\"day_of_month\"] = df.index.day\n",
    "    df[\"month\"] = df.index.month\n",
    "    holidays_swe = holidays.Sweden(include_sundays=False)[df.index[0]: df.index[-1]]\n",
    "    df[\"holiday\"] = [1 if d in holidays_swe else 0 for d in df.index.date]\n",
    "\n",
    "def remove_zero_sequence(df, col, min_length):\n",
    "    mask = col.groupby((col != col.shift()).cumsum()).transform('count').lt(min_length)\n",
    "    mask = ~(mask | col.gt(0))\n",
    "    df.drop(mask[mask].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_split(df, train_len=pd.Timedelta(4, unit='W'), forecast_len=pd.Timedelta(1, unit='W'), expanding_window=False):\n",
    "    start = df.index.values[0]\n",
    "    split = start + train_len\n",
    "    forecast_end = split + forecast_len\n",
    "    end = df.index.values[-1]\n",
    "    sets = []\n",
    "    while end > forecast_end:\n",
    "        sets.append((df[start:split].index, df[split:forecast_end].index))\n",
    "        if not expanding_window:\n",
    "            start += train_len + forecast_len\n",
    "        split += train_len + forecast_len\n",
    "        forecast_end += train_len + forecast_len\n",
    "    return sets\n",
    "\n",
    "DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == DEVICES[0][0], COL_WL].copy()\n",
    "len(get_test_train_split(DEVICE_FRAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "def train_and_predict_var(train_set, n_steps):\n",
    "    model = VAR(endog=train_set)\n",
    "    model_fit = model.fit(maxlags=2, trend=\"nc\")\n",
    "    print(model_fit.y)\n",
    "    return\n",
    "    prediction = model_fit.forecast(model_fit.y, steps=n_steps)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "\n",
    "def validate(predictions, true_val):\n",
    "    sum_meanAE = mean_absolute_error(predictions, true_val).sum()\n",
    "    sum_medianAE = median_absolute_error(predictions, true_val).sum()\n",
    "    return sum_meanAE, sum_medianAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_FRAME = TSF.loc[TSF[\"device_id\"] == DEVICES[0][0], COL_WL].copy()\n",
    "#DEVICE_FRAME[\"orders\"] = 1\n",
    "AGG_DICT = {col_name:np.sum for col_name in DEVICE_FRAME.columns}\n",
    "AGG_DICT.update({\"total_price\":np.mean})\n",
    "DEVICE_FRAME = DEVICE_FRAME.resample(\"H\").agg(AGG_DICT).fillna(0).cumsum()\n",
    "DEVICE_FRAME.rename(columns={'total_price': 'average_price'}, inplace=True)\n",
    "DEVICE_FRAME.drop([\"average_price\"], axis=1, inplace=True)\n",
    "#remove_zero_sequence(DEVICE_FRAME, DEVICE_FRAME.orders, 25)\n",
    "#add_time_cols(DEVICE_FRAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_train = get_test_train_split(DEVICE_FRAME)\n",
    "pred = train_and_predict_var(DEVICE_FRAME.loc[test_train[0][0]], len(test_train[0][1]))\n",
    "validate(pred, DEVICE_FRAME.loc[test_train[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUM_TRUE = DEVICE_FRAME.loc[test_train[0][1]]#DEVICE_FRAME.loc[test_train[0][0]].append(DEVICE_FRAME.loc[test_train[0][1]])\n",
    "PRED = pd.DataFrame(index=SUM_TRUE.index,columns=SUM_TRUE.columns)\n",
    "for j in range(0,len(DEVICE_FRAME.loc[test_train[0][1]].columns)):\n",
    "    for i in range(0, len(pred)):\n",
    "       PRED.iloc[i][j] = pred[i][j]\n",
    "SUM_VAR = PRED\n",
    "COLS = get_top_cols(SUM_TRUE, -1, 5)\n",
    "COLOR_DICT = get_color_dict(COLS)\n",
    "_, ax = plt.subplots()\n",
    "SUM_TRUE.plot(ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "SUM_VAR.plot(linestyle='dashed', ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "plt.legend(prop={'size': 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, test in get_test_train_split(DEVICE_FRAME, pd.Timedelta(4, unit='W'), pd.Timedelta(1, unit='W')):\n",
    "    train = DEVICE_FRAME.loc[train]\n",
    "    test = DEVICE_FRAME.loc[test]#-train.iloc[-1]\n",
    "    pred = train_and_predict_var(train, len(test))\n",
    "    #validate(pred, test)\n",
    "\n",
    "    SUM_TRUE = test#DEVICE_FRAME.loc[test_train[0][0]].append(DEVICE_FRAME.loc[test_train[0][1]])\n",
    "    PRED = pd.DataFrame(index=test.index,columns=test.columns)\n",
    "    for j in range(0,len(test.columns)):\n",
    "        for i in range(0, len(pred)):\n",
    "            PRED.iloc[i][j] = pred[i][j]\n",
    "    SUM_VAR = PRED#-train.iloc[-1]\n",
    "    COLS = get_top_cols(SUM_TRUE, -1, 5)\n",
    "    COLOR_DICT = get_color_dict(COLS)\n",
    "    _, ax = plt.subplots()\n",
    "    train.append(SUM_TRUE).plot(ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "    SUM_VAR.plot(linestyle='dashed', ax=ax, kind='line',y=COLS, figsize=(20, 14), color=[COLOR_DICT.get(x, '#FFFFFF') for x in COLS], linewidth=3.0)\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.scatter(DEVICE_FRAME.index, DEVICE_FRAME[\"orders\"].cumsum())\n",
    "plt.figure(figsize=(20,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN_PERCENT = 0.8\n",
    "TRAIN_SIZE = -336#int(len(DEVICE_FRAME)*TRAIN_PERCENT)\n",
    "TRAIN_Y = DEVICE_FRAME[:TRAIN_SIZE]\n",
    "TRAIN_Y = TRAIN_Y.loc[:, (TRAIN_Y != TRAIN_Y.iloc[0]).any()]\n",
    "VAL_Y = DEVICE_FRAME[TRAIN_Y.columns][TRAIN_SIZE:]\n",
    "#TRAIN_X = DEVICE_FRAME[\"day\"]\n",
    "#VAL_X = DEVICE_FRAME[\"day\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#creating the train and validation set\n",
    "train = TRAIN_Y\n",
    "valid = VAL_Y\n",
    "naive_pred = DEVICE_FRAME[TRAIN_SIZE-1:-1]\n",
    "\n",
    "#fit the model\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "model = VAR(endog=train)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# make prediction on validation\n",
    "prediction = model_fit.forecast(model_fit.y, steps=len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=valid.index,columns=VAL_Y.columns)\n",
    "for j in range(0,len(VAL_Y.columns)):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "#check rmse\n",
    "print(\"MAE VAR, MAE Naive, Min Val, Max Val\")\n",
    "MIN = valid.min()\n",
    "MAX = valid.max()\n",
    "SUM_MAE = 0\n",
    "for i in pred.columns:\n",
    "    SUM_MAE += mean_absolute_error(pred[i], valid[i])\n",
    "    print('MAE value for', str(i)+':\\t', mean_absolute_error(pred[i], valid[i]), \"\\t\", mean_absolute_error(naive_pred[i], valid[i]), \"\\t\", MIN[i], \"\\t\", MAX[i])\n",
    "print(\"AVG MAE:\", (SUM_MAE/len(VAL_Y.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#converting predictions to dataframe\n",
    "pred = pd.DataFrame(index=valid.index,columns=DEVICE_FRAME.columns)\n",
    "for j in range(0,len(DEVICE_FRAME.columns)):\n",
    "    for i in range(0, len(prediction)):\n",
    "       pred.iloc[i][j] = prediction[i][j]\n",
    "\n",
    "#check rmse\n",
    "print(\"RMSE for VAR predictions and Naive Predictions\")\n",
    "for i in DEVICE_FRAME.columns:\n",
    "    print('rmse value for', i, 'is : ', sqrt(mean_squared_error(pred[i], valid[i])), \"\\t\", sqrt(mean_squared_error(naive_pred[i], valid[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "DEVICE_FRAME = sc.fit_transform(DEVICE_FRAME)\n",
    "DEVICE_FRAME.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}